{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import Input\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 8192, 2)\n"
     ]
    }
   ],
   "source": [
    "state0=np.load('../Data/new/2gsps/state0.npy')\n",
    "state1=np.load('../Data/new/2gsps/state1.npy')\n",
    "output0=np.zeros((state0.shape[0]))\n",
    "output1=np.ones((state1.shape[0]))\n",
    "print(state0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 8192, 2)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "x=np.vstack((state0,state1))\n",
    "print(x.shape)\n",
    "y=np.hstack((output0,output1))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1400, 2000, 2)\n",
      "704.0\n",
      "(1400, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "st,ed=700,2700\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:,st:ed,:], y, test_size=0.30, random_state=45)\n",
    "print(x_train.shape)\n",
    "print(y_train.sum())\n",
    "\n",
    "x_train=np.mean(x_train,axis=1)\n",
    "x_test=np.mean(x_test,axis=1)\n",
    "\n",
    "# x_train=x_train.reshape((x_train.shape[0],x_train.shape[1]*2))\n",
    "# x_test=x_test.reshape((x_test.shape[0],x_test.shape[1]*2))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7833333333333333\n"
     ]
    }
   ],
   "source": [
    "# x_trainC=x_train.reshape((x_train.shape[0],x_train.shape[1]*2))\n",
    "# x_testC=x_test.reshape((x_test.shape[0],x_test.shape[1]*2))\n",
    "classifier = LogisticRegression(random_state = 42, max_iter=500)\n",
    "classifier.fit(x_train, y_train)\n",
    "print(classifier.score(x_test,y_test))\n",
    "pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.795\n"
     ]
    }
   ],
   "source": [
    "model=SVC(decision_function_shape='ovo')\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(0|0)= 0.758\n",
      "P(1|1)= 0.82\n"
     ]
    }
   ],
   "source": [
    "Y=list(zip(np.append(np.mean(state0[:,st:ed,0],axis=1),np.mean(state1[:,st:ed,0],axis=1)),np.append(np.mean(state0[:,st:ed,1],axis=1),np.mean(state1[:,st:ed,1],axis=1))))\n",
    "gmm_Y=GaussianMixture(n_components=2,covariance_type='full').fit(Y)\n",
    "bitstring=np.split(gmm_Y.predict(Y),2)\n",
    "print('P(0|0)=',len(bitstring[0][bitstring[0]==0])/len(bitstring[0]))\n",
    "print('P(1|1)=',len(bitstring[1][bitstring[1]==1])/len(bitstring[1]))\n",
    "X=list(zip(np.mean(state0[:,st:ed,0],axis=1),np.mean(state0[:,st:ed,1],axis=1)))\n",
    "gmm_Y_1=GaussianMixture(n_components=1,covariance_type='spherical').fit(X)\n",
    "X=list(zip(np.mean(state1[:,st:ed,0],axis=1),np.mean(state1[:,st:ed,1],axis=1)))\n",
    "gmm_Y_2=GaussianMixture(n_components=1,covariance_type='spherical').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_6 (Dense)             (None, 32)                96        \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 641 (2.50 KB)\n",
      "Trainable params: 641 (2.50 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def arch():\n",
    "    model=Sequential()\n",
    "    model.add(Input(shape=(2)))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    # model.add(Dense(16,activation='relu'))\n",
    "    # # model.add(Dropout(0.8))\n",
    "    # model.add(Dense(16,activation='relu'))\n",
    "    # # model.add(Dropout(0.8))\n",
    "    # model.add(Dense(32,activation='relu'))\n",
    "    # # model.add(Dropout(0.8))\n",
    "    # model.add(Dense(16,activation='relu'))\n",
    "    # # model.add(Dropout(0.8))\n",
    "    # model.add(Dense(4,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    return model\n",
    "arch().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def bceloss(y_true, y_pred):\n",
    "    # Clip the prediction value to prevent log(0) error\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # Calculate the weighted binary cross entropy loss\n",
    "    loss = -(0.1 * (y_true * K.log(y_pred)) + 0.9*((1 - y_true) * K.log(1 - y_pred)))\n",
    "    return K.mean(loss, axis=-1)\n",
    "\n",
    "model=arch()\n",
    "model.summary\n",
    "opt=SGD( learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model_path=\"../Model/NN2/{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "60/88 [===================>..........] - ETA: 0s - loss: 0.6729 - accuracy: 0.6177 \n",
      "Epoch 1: val_accuracy improved from -inf to 0.90500, saving model to ../Model/NN2/01-0.9050.h5\n",
      "88/88 [==============================] - 1s 3ms/step - loss: 0.6365 - accuracy: 0.6486 - val_loss: 0.3724 - val_accuracy: 0.9050\n",
      "Epoch 2/100\n",
      "67/88 [=====================>........] - ETA: 0s - loss: 0.4214 - accuracy: 0.7854\n",
      "Epoch 2: val_accuracy improved from 0.90500 to 0.90667, saving model to ../Model/NN2/02-0.9067.h5\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.4078 - accuracy: 0.7993 - val_loss: 0.3105 - val_accuracy: 0.9067\n",
      "Epoch 3/100\n",
      " 1/88 [..............................] - ETA: 0s - loss: 0.4330 - accuracy: 0.7500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/n/nrvora/.conda/envs/readout/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/88 [=====================>........] - ETA: 0s - loss: 0.3502 - accuracy: 0.8610\n",
      "Epoch 3: val_accuracy improved from 0.90667 to 0.91000, saving model to ../Model/NN2/03-0.9100.h5\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.3445 - accuracy: 0.8664 - val_loss: 0.2936 - val_accuracy: 0.9100\n",
      "Epoch 4/100\n",
      "70/88 [======================>.......] - ETA: 0s - loss: 0.3020 - accuracy: 0.9089\n",
      "Epoch 4: val_accuracy improved from 0.91000 to 0.91167, saving model to ../Model/NN2/04-0.9117.h5\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.3094 - accuracy: 0.9071 - val_loss: 0.2934 - val_accuracy: 0.9117\n",
      "Epoch 5/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2849 - accuracy: 0.9190\n",
      "Epoch 5: val_accuracy did not improve from 0.91167\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2838 - accuracy: 0.9164 - val_loss: 0.2985 - val_accuracy: 0.9117\n",
      "Epoch 6/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2675 - accuracy: 0.9199\n",
      "Epoch 6: val_accuracy did not improve from 0.91167\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2732 - accuracy: 0.9171 - val_loss: 0.2989 - val_accuracy: 0.9117\n",
      "Epoch 7/100\n",
      "66/88 [=====================>........] - ETA: 0s - loss: 0.2615 - accuracy: 0.9347\n",
      "Epoch 7: val_accuracy improved from 0.91167 to 0.91333, saving model to ../Model/NN2/07-0.9133.h5\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2679 - accuracy: 0.9343 - val_loss: 0.2998 - val_accuracy: 0.9133\n",
      "Epoch 8/100\n",
      "67/88 [=====================>........] - ETA: 0s - loss: 0.2969 - accuracy: 0.9235\n",
      "Epoch 8: val_accuracy improved from 0.91333 to 0.91500, saving model to ../Model/NN2/08-0.9150.h5\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.9243 - val_loss: 0.2948 - val_accuracy: 0.9150\n",
      "Epoch 9/100\n",
      "69/88 [======================>.......] - ETA: 0s - loss: 0.2718 - accuracy: 0.9312\n",
      "Epoch 9: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2701 - accuracy: 0.9307 - val_loss: 0.2992 - val_accuracy: 0.9150\n",
      "Epoch 10/100\n",
      "68/88 [======================>.......] - ETA: 0s - loss: 0.2517 - accuracy: 0.9375\n",
      "Epoch 10: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.9343 - val_loss: 0.3039 - val_accuracy: 0.9117\n",
      "Epoch 11/100\n",
      "69/88 [======================>.......] - ETA: 0s - loss: 0.2603 - accuracy: 0.9312\n",
      "Epoch 11: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.9286 - val_loss: 0.2971 - val_accuracy: 0.9150\n",
      "Epoch 12/100\n",
      "70/88 [======================>.......] - ETA: 0s - loss: 0.2586 - accuracy: 0.9375\n",
      "Epoch 12: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.9364 - val_loss: 0.2973 - val_accuracy: 0.9150\n",
      "Epoch 13/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2610 - accuracy: 0.9332\n",
      "Epoch 13: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2634 - accuracy: 0.9329 - val_loss: 0.2994 - val_accuracy: 0.9150\n",
      "Epoch 14/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2670 - accuracy: 0.9274\n",
      "Epoch 14: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2565 - accuracy: 0.9286 - val_loss: 0.3002 - val_accuracy: 0.9133\n",
      "Epoch 15/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2886 - accuracy: 0.9231\n",
      "Epoch 15: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.9264 - val_loss: 0.2961 - val_accuracy: 0.9100\n",
      "Epoch 16/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2675 - accuracy: 0.9298\n",
      "Epoch 16: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2564 - accuracy: 0.9336 - val_loss: 0.2970 - val_accuracy: 0.9150\n",
      "Epoch 17/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2788 - accuracy: 0.9292\n",
      "Epoch 17: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2679 - accuracy: 0.9307 - val_loss: 0.2978 - val_accuracy: 0.9117\n",
      "Epoch 18/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2505 - accuracy: 0.9317\n",
      "Epoch 18: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.9321 - val_loss: 0.3022 - val_accuracy: 0.9100\n",
      "Epoch 19/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2321 - accuracy: 0.9401\n",
      "Epoch 19: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2476 - accuracy: 0.9350 - val_loss: 0.3042 - val_accuracy: 0.9150\n",
      "Epoch 20/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2497 - accuracy: 0.9350\n",
      "Epoch 20: val_accuracy did not improve from 0.91500\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2473 - accuracy: 0.9357 - val_loss: 0.3043 - val_accuracy: 0.9117\n",
      "Epoch 21/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2658 - accuracy: 0.9315\n",
      "Epoch 21: val_accuracy improved from 0.91500 to 0.91667, saving model to ../Model/NN2/21-0.9167.h5\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2545 - accuracy: 0.9329 - val_loss: 0.3015 - val_accuracy: 0.9167\n",
      "Epoch 22/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2479 - accuracy: 0.9358\n",
      "Epoch 22: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9386 - val_loss: 0.3013 - val_accuracy: 0.9117\n",
      "Epoch 23/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2549 - accuracy: 0.9349\n",
      "Epoch 23: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2514 - accuracy: 0.9357 - val_loss: 0.2989 - val_accuracy: 0.9117\n",
      "Epoch 24/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2435 - accuracy: 0.9282\n",
      "Epoch 24: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2485 - accuracy: 0.9279 - val_loss: 0.2959 - val_accuracy: 0.9117\n",
      "Epoch 25/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2611 - accuracy: 0.9248\n",
      "Epoch 25: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2599 - accuracy: 0.9257 - val_loss: 0.2959 - val_accuracy: 0.9150\n",
      "Epoch 26/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2436 - accuracy: 0.9267\n",
      "Epoch 26: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.9279 - val_loss: 0.2989 - val_accuracy: 0.9167\n",
      "Epoch 27/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2529 - accuracy: 0.9358\n",
      "Epoch 27: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2458 - accuracy: 0.9371 - val_loss: 0.2985 - val_accuracy: 0.9133\n",
      "Epoch 28/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2297 - accuracy: 0.9375\n",
      "Epoch 28: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.9379 - val_loss: 0.3061 - val_accuracy: 0.9117\n",
      "Epoch 29/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2376 - accuracy: 0.9367\n",
      "Epoch 29: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2415 - accuracy: 0.9364 - val_loss: 0.3039 - val_accuracy: 0.9117\n",
      "Epoch 30/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2408 - accuracy: 0.9341\n",
      "Epoch 30: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9321 - val_loss: 0.3025 - val_accuracy: 0.9117\n",
      "Epoch 31/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2384 - accuracy: 0.9367\n",
      "Epoch 31: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2403 - accuracy: 0.9350 - val_loss: 0.3022 - val_accuracy: 0.9117\n",
      "Epoch 32/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2448 - accuracy: 0.9392\n",
      "Epoch 32: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2380 - accuracy: 0.9379 - val_loss: 0.3003 - val_accuracy: 0.9167\n",
      "Epoch 33/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2264 - accuracy: 0.9350\n",
      "Epoch 33: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9343 - val_loss: 0.3010 - val_accuracy: 0.9133\n",
      "Epoch 34/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2355 - accuracy: 0.9341\n",
      "Epoch 34: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.9336 - val_loss: 0.3035 - val_accuracy: 0.9167\n",
      "Epoch 35/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2451 - accuracy: 0.9287\n",
      "Epoch 35: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9336 - val_loss: 0.3009 - val_accuracy: 0.9117\n",
      "Epoch 36/100\n",
      "70/88 [======================>.......] - ETA: 0s - loss: 0.2526 - accuracy: 0.9375\n",
      "Epoch 36: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.9329 - val_loss: 0.3048 - val_accuracy: 0.9117\n",
      "Epoch 37/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2128 - accuracy: 0.9410\n",
      "Epoch 37: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2275 - accuracy: 0.9386 - val_loss: 0.3004 - val_accuracy: 0.9117\n",
      "Epoch 38/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2269 - accuracy: 0.9332\n",
      "Epoch 38: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2239 - accuracy: 0.9343 - val_loss: 0.3018 - val_accuracy: 0.9133\n",
      "Epoch 39/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2343 - accuracy: 0.9349\n",
      "Epoch 39: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2320 - accuracy: 0.9343 - val_loss: 0.3031 - val_accuracy: 0.9117\n",
      "Epoch 40/100\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 0.2399 - accuracy: 0.9383\n",
      "Epoch 40: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2480 - accuracy: 0.9400 - val_loss: 0.2991 - val_accuracy: 0.9117\n",
      "Epoch 41/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2596 - accuracy: 0.9358\n",
      "Epoch 41: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2583 - accuracy: 0.9364 - val_loss: 0.2920 - val_accuracy: 0.9167\n",
      "Epoch 42/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2402 - accuracy: 0.9367\n",
      "Epoch 42: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.9379 - val_loss: 0.2933 - val_accuracy: 0.9117\n",
      "Epoch 43/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2461 - accuracy: 0.9367\n",
      "Epoch 43: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2443 - accuracy: 0.9357 - val_loss: 0.2940 - val_accuracy: 0.9117\n",
      "Epoch 44/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2307 - accuracy: 0.9326\n",
      "Epoch 44: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2313 - accuracy: 0.9336 - val_loss: 0.3002 - val_accuracy: 0.9100\n",
      "Epoch 45/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2441 - accuracy: 0.9341\n",
      "Epoch 45: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9350 - val_loss: 0.2963 - val_accuracy: 0.9117\n",
      "Epoch 46/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2470 - accuracy: 0.9375\n",
      "Epoch 46: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2505 - accuracy: 0.9357 - val_loss: 0.2957 - val_accuracy: 0.9117\n",
      "Epoch 47/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2444 - accuracy: 0.9358\n",
      "Epoch 47: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2416 - accuracy: 0.9357 - val_loss: 0.2966 - val_accuracy: 0.9117\n",
      "Epoch 48/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2315 - accuracy: 0.9391\n",
      "Epoch 48: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.9364 - val_loss: 0.2958 - val_accuracy: 0.9150\n",
      "Epoch 49/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2251 - accuracy: 0.9343\n",
      "Epoch 49: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9364 - val_loss: 0.3017 - val_accuracy: 0.9083\n",
      "Epoch 50/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2597 - accuracy: 0.9327\n",
      "Epoch 50: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2523 - accuracy: 0.9336 - val_loss: 0.2951 - val_accuracy: 0.9100\n",
      "Epoch 51/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2434 - accuracy: 0.9375\n",
      "Epoch 51: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9343 - val_loss: 0.2947 - val_accuracy: 0.9117\n",
      "Epoch 52/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2377 - accuracy: 0.9359\n",
      "Epoch 52: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2405 - accuracy: 0.9336 - val_loss: 0.2991 - val_accuracy: 0.9133\n",
      "Epoch 53/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2440 - accuracy: 0.9335\n",
      "Epoch 53: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9350 - val_loss: 0.2973 - val_accuracy: 0.9133\n",
      "Epoch 54/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2596 - accuracy: 0.9351\n",
      "Epoch 54: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2504 - accuracy: 0.9393 - val_loss: 0.2917 - val_accuracy: 0.9133\n",
      "Epoch 55/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2488 - accuracy: 0.9350\n",
      "Epoch 55: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2446 - accuracy: 0.9350 - val_loss: 0.2954 - val_accuracy: 0.9133\n",
      "Epoch 56/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2435 - accuracy: 0.9310\n",
      "Epoch 56: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.9329 - val_loss: 0.2962 - val_accuracy: 0.9083\n",
      "Epoch 57/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2264 - accuracy: 0.9343\n",
      "Epoch 57: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9357 - val_loss: 0.2999 - val_accuracy: 0.9100\n",
      "Epoch 58/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2339 - accuracy: 0.9383\n",
      "Epoch 58: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2234 - accuracy: 0.9407 - val_loss: 0.3015 - val_accuracy: 0.9117\n",
      "Epoch 59/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2401 - accuracy: 0.9383\n",
      "Epoch 59: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9379 - val_loss: 0.3002 - val_accuracy: 0.9150\n",
      "Epoch 60/100\n",
      "70/88 [======================>.......] - ETA: 0s - loss: 0.2409 - accuracy: 0.9321\n",
      "Epoch 60: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2364 - accuracy: 0.9336 - val_loss: 0.2979 - val_accuracy: 0.9117\n",
      "Epoch 61/100\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 0.2192 - accuracy: 0.9359\n",
      "Epoch 61: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2153 - accuracy: 0.9400 - val_loss: 0.3030 - val_accuracy: 0.9133\n",
      "Epoch 62/100\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 0.2304 - accuracy: 0.9350\n",
      "Epoch 62: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2290 - accuracy: 0.9379 - val_loss: 0.2960 - val_accuracy: 0.9133\n",
      "Epoch 63/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2365 - accuracy: 0.9367\n",
      "Epoch 63: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2390 - accuracy: 0.9379 - val_loss: 0.2944 - val_accuracy: 0.9117\n",
      "Epoch 64/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2381 - accuracy: 0.9391\n",
      "Epoch 64: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2328 - accuracy: 0.9386 - val_loss: 0.2942 - val_accuracy: 0.9133\n",
      "Epoch 65/100\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 0.2190 - accuracy: 0.9383\n",
      "Epoch 65: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2282 - accuracy: 0.9357 - val_loss: 0.2959 - val_accuracy: 0.9117\n",
      "Epoch 66/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2399 - accuracy: 0.9319\n",
      "Epoch 66: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9350 - val_loss: 0.2949 - val_accuracy: 0.9133\n",
      "Epoch 67/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2243 - accuracy: 0.9328\n",
      "Epoch 67: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9307 - val_loss: 0.2987 - val_accuracy: 0.9100\n",
      "Epoch 68/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2284 - accuracy: 0.9343\n",
      "Epoch 68: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2336 - accuracy: 0.9357 - val_loss: 0.2965 - val_accuracy: 0.9100\n",
      "Epoch 69/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2423 - accuracy: 0.9312\n",
      "Epoch 69: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2365 - accuracy: 0.9350 - val_loss: 0.2920 - val_accuracy: 0.9133\n",
      "Epoch 70/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2391 - accuracy: 0.9316\n",
      "Epoch 70: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9350 - val_loss: 0.2937 - val_accuracy: 0.9117\n",
      "Epoch 71/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2391 - accuracy: 0.9393\n",
      "Epoch 71: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2412 - accuracy: 0.9393 - val_loss: 0.2936 - val_accuracy: 0.9133\n",
      "Epoch 72/100\n",
      "70/88 [======================>.......] - ETA: 0s - loss: 0.2316 - accuracy: 0.9321\n",
      "Epoch 72: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2420 - accuracy: 0.9314 - val_loss: 0.2949 - val_accuracy: 0.9117\n",
      "Epoch 73/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2579 - accuracy: 0.9351\n",
      "Epoch 73: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2550 - accuracy: 0.9350 - val_loss: 0.2917 - val_accuracy: 0.9133\n",
      "Epoch 74/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2215 - accuracy: 0.9408\n",
      "Epoch 74: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9343 - val_loss: 0.2941 - val_accuracy: 0.9133\n",
      "Epoch 75/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2307 - accuracy: 0.9333\n",
      "Epoch 75: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2269 - accuracy: 0.9357 - val_loss: 0.2949 - val_accuracy: 0.9167\n",
      "Epoch 76/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2430 - accuracy: 0.9288\n",
      "Epoch 76: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9350 - val_loss: 0.2979 - val_accuracy: 0.9133\n",
      "Epoch 77/100\n",
      "73/88 [=======================>......] - ETA: 0s - loss: 0.2452 - accuracy: 0.9281\n",
      "Epoch 77: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.9343 - val_loss: 0.2940 - val_accuracy: 0.9117\n",
      "Epoch 78/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2465 - accuracy: 0.9287\n",
      "Epoch 78: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9307 - val_loss: 0.2955 - val_accuracy: 0.9083\n",
      "Epoch 79/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2212 - accuracy: 0.9408\n",
      "Epoch 79: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9407 - val_loss: 0.2937 - val_accuracy: 0.9150\n",
      "Epoch 80/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2276 - accuracy: 0.9383\n",
      "Epoch 80: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9386 - val_loss: 0.2983 - val_accuracy: 0.9117\n",
      "Epoch 81/100\n",
      "77/88 [=========================>....] - ETA: 0s - loss: 0.2413 - accuracy: 0.9375\n",
      "Epoch 81: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9393 - val_loss: 0.2976 - val_accuracy: 0.9117\n",
      "Epoch 82/100\n",
      "78/88 [=========================>....] - ETA: 0s - loss: 0.2416 - accuracy: 0.9415\n",
      "Epoch 82: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2435 - accuracy: 0.9400 - val_loss: 0.2959 - val_accuracy: 0.9117\n",
      "Epoch 83/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2385 - accuracy: 0.9367\n",
      "Epoch 83: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2344 - accuracy: 0.9379 - val_loss: 0.2953 - val_accuracy: 0.9150\n",
      "Epoch 84/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2288 - accuracy: 0.9359\n",
      "Epoch 84: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2368 - accuracy: 0.9350 - val_loss: 0.2965 - val_accuracy: 0.9133\n",
      "Epoch 85/100\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 0.2405 - accuracy: 0.9350\n",
      "Epoch 85: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9350 - val_loss: 0.2938 - val_accuracy: 0.9117\n",
      "Epoch 86/100\n",
      "79/88 [=========================>....] - ETA: 0s - loss: 0.2229 - accuracy: 0.9454\n",
      "Epoch 86: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2332 - accuracy: 0.9407 - val_loss: 0.2939 - val_accuracy: 0.9117\n",
      "Epoch 87/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2304 - accuracy: 0.9375\n",
      "Epoch 87: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2256 - accuracy: 0.9393 - val_loss: 0.2927 - val_accuracy: 0.9117\n",
      "Epoch 88/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2209 - accuracy: 0.9350\n",
      "Epoch 88: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2243 - accuracy: 0.9379 - val_loss: 0.2952 - val_accuracy: 0.9100\n",
      "Epoch 89/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2245 - accuracy: 0.9383\n",
      "Epoch 89: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2228 - accuracy: 0.9400 - val_loss: 0.2952 - val_accuracy: 0.9117\n",
      "Epoch 90/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2393 - accuracy: 0.9306\n",
      "Epoch 90: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9329 - val_loss: 0.2971 - val_accuracy: 0.9083\n",
      "Epoch 91/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2311 - accuracy: 0.9418\n",
      "Epoch 91: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9379 - val_loss: 0.2990 - val_accuracy: 0.9117\n",
      "Epoch 92/100\n",
      "71/88 [=======================>......] - ETA: 0s - loss: 0.2254 - accuracy: 0.9384\n",
      "Epoch 92: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2259 - accuracy: 0.9379 - val_loss: 0.2958 - val_accuracy: 0.9117\n",
      "Epoch 93/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2194 - accuracy: 0.9367\n",
      "Epoch 93: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9364 - val_loss: 0.2978 - val_accuracy: 0.9100\n",
      "Epoch 94/100\n",
      "74/88 [========================>.....] - ETA: 0s - loss: 0.2396 - accuracy: 0.9409\n",
      "Epoch 94: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2372 - accuracy: 0.9400 - val_loss: 0.2940 - val_accuracy: 0.9150\n",
      "Epoch 95/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2176 - accuracy: 0.9392\n",
      "Epoch 95: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2128 - accuracy: 0.9393 - val_loss: 0.2978 - val_accuracy: 0.9117\n",
      "Epoch 96/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2254 - accuracy: 0.9358\n",
      "Epoch 96: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9371 - val_loss: 0.2949 - val_accuracy: 0.9117\n",
      "Epoch 97/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2265 - accuracy: 0.9358\n",
      "Epoch 97: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.9371 - val_loss: 0.2972 - val_accuracy: 0.9133\n",
      "Epoch 98/100\n",
      "76/88 [========================>.....] - ETA: 0s - loss: 0.2285 - accuracy: 0.9350\n",
      "Epoch 98: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2389 - accuracy: 0.9329 - val_loss: 0.2984 - val_accuracy: 0.9117\n",
      "Epoch 99/100\n",
      "75/88 [========================>.....] - ETA: 0s - loss: 0.2472 - accuracy: 0.9350\n",
      "Epoch 99: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2450 - accuracy: 0.9371 - val_loss: 0.2957 - val_accuracy: 0.9117\n",
      "Epoch 100/100\n",
      "72/88 [=======================>......] - ETA: 0s - loss: 0.2085 - accuracy: 0.9410\n",
      "Epoch 100: val_accuracy did not improve from 0.91667\n",
      "88/88 [==============================] - 0s 2ms/step - loss: 0.2215 - accuracy: 0.9329 - val_loss: 0.3002 - val_accuracy: 0.9133\n"
     ]
    }
   ],
   "source": [
    "H=model.fit(x_train,y_train,\n",
    "          validation_data=(x_test, y_test),\n",
    "          epochs=100,batch_size=16,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 0s 586us/step\n",
      "304 283\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "count1=0\n",
    "y_pred=model.predict(x_test)\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_test[i]==0:\n",
    "        count+=1\n",
    "        if y_pred[i,0]<0.5:\n",
    "            count1+=1\n",
    "print(count,count1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GaussianDiscriminantAnalysis:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = np.array([np.mean(y == c) for c in self.classes])\n",
    "\n",
    "        self.means = []\n",
    "        self.cov_matrices = []\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            cov_matrix_c = np.cov(X_c, rowvar=False)\n",
    "            self.means.append(mean_c)\n",
    "            self.cov_matrices.append(cov_matrix_c)\n",
    "\n",
    "    def predict(self, X):\n",
    "        posteriors = []\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_prior = self.class_priors[int(c)]\n",
    "            mean = self.means[int(c)]\n",
    "            cov_matrix = self.cov_matrices[int(c)]\n",
    "            mvn = multivariate_normal(mean=mean, cov=cov_matrix)\n",
    "            posterior = class_prior * mvn.pdf(X)\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        posteriors = np.array(posteriors).T\n",
    "        predicted_labels = np.argmax(posteriors, axis=1)\n",
    "\n",
    "        return predicted_labels\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and train GDA\n",
    "gda = GaussianDiscriminantAnalysis()\n",
    "gda.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gda.predict(x_train)\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NERSC Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
