{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 11:10:45.221951: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-25 11:10:46.215068: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,load_model\n",
    "from keras import layers,Input\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-25 11:10:49.404383: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 2)\n",
      "[[[ 1 11]\n",
      "  [ 2 12]\n",
      "  [ 3 13]\n",
      "  [ 4 14]\n",
      "  [ 5 15]\n",
      "  [ 6 16]\n",
      "  [ 7 17]\n",
      "  [ 8 18]\n",
      "  [ 9 19]\n",
      "  [10 20]]\n",
      "\n",
      " [[ 1 11]\n",
      "  [ 2 12]\n",
      "  [ 3 13]\n",
      "  [ 4 14]\n",
      "  [ 5 15]\n",
      "  [ 6 16]\n",
      "  [ 7 17]\n",
      "  [ 8 18]\n",
      "  [ 9 19]\n",
      "  [10 20]]]\n",
      "[[[ 1 11  2 12]\n",
      "  [ 3 13  4 14]\n",
      "  [ 5 15  6 16]\n",
      "  [ 7 17  8 18]\n",
      "  [ 9 19 10 20]]\n",
      "\n",
      " [[ 1 11  2 12]\n",
      "  [ 3 13  4 14]\n",
      "  [ 5 15  6 16]\n",
      "  [ 7 17  8 18]\n",
      "  [ 9 19 10 20]]]\n"
     ]
    }
   ],
   "source": [
    "arry=np.array([[[1,11],[2,12],[3,13],[4,14],[5,15],[6,16],[7,17],[8,18],[9,19],[10,20]],[[1,11],[2,12],[3,13],[4,14],[5,15],[6,16],[7,17],[8,18],[9,19],[10,20]]])\n",
    "# arry=np.array([[[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19,20]],[[1,2,3,4,5,6,7,8,9,10],[11,12,13,14,15,16,17,18,19,20]]])\n",
    "print(arry.shape)\n",
    "print(arry)\n",
    "print(arry.reshape((2,5,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 8192, 2)\n"
     ]
    }
   ],
   "source": [
    "state0=np.load('../Data/new/2gsps/state0.npy')\n",
    "state1=np.load('../Data/new/2gsps/state1.npy')\n",
    "output0=np.zeros((state0.shape[0]))\n",
    "output1=np.ones((state1.shape[0]))\n",
    "print(state0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12000, 8192, 2)\n",
      "(12000,)\n"
     ]
    }
   ],
   "source": [
    "x=np.vstack((state0,state1))\n",
    "print(x.shape)\n",
    "y=np.hstack((output0,output1))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8400, 3000, 2)\n",
      "4233.0\n",
      "(8400, 2)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "st,ed=700,3700\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:,st:ed,:], y, test_size=0.30, random_state=45)\n",
    "print(x_train.shape)\n",
    "print(y_train.sum())\n",
    "\n",
    "x_train=np.mean(x_train,axis=1)\n",
    "x_test=np.mean(x_test,axis=1)\n",
    "\n",
    "# x_train=x_train.reshape((x_train.shape[0],x_train.shape[1]*2))\n",
    "# x_test=x_test.reshape((x_test.shape[0],x_test.shape[1]*2))\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# x_trainC=x_train.reshape((x_train.shape[0],x_train.shape[1]*2))\n",
    "# x_testC=x_test.reshape((x_test.shape[0],x_test.shape[1]*2))\n",
    "classifier = LogisticRegression(random_state = 42, max_iter=500)\n",
    "classifier.fit(x_train, y_train)\n",
    "print(classifier.score(x_test,y_test))\n",
    "pred = classifier.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9661111111111111\n"
     ]
    }
   ],
   "source": [
    "model=SVC(decision_function_shape='ovo')\n",
    "model.fit(x_train,y_train)\n",
    "print(model.score(x_test,y_test))\n",
    "pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/global/homes/n/nrvora/Projects/Quantum/Code/ml.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e45525343227d/global/homes/n/nrvora/Projects/Quantum/Code/ml.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m Y\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(state0[:,st:ed,\u001b[39m0\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),np\u001b[39m.\u001b[39mmean(state1[:,st:ed,\u001b[39m0\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)),np\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mmean(state0[:,st:ed,\u001b[39m1\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m),np\u001b[39m.\u001b[39mmean(state1[:,st:ed,\u001b[39m1\u001b[39m],axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))))\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e45525343227d/global/homes/n/nrvora/Projects/Quantum/Code/ml.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(Y)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a224e45525343227d/global/homes/n/nrvora/Projects/Quantum/Code/ml.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m gmm_Y\u001b[39m=\u001b[39mGaussianMixture(n_components\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,covariance_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mfit(Y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "Y=list(zip(np.append(np.mean(state0[:,st:ed,0],axis=1),np.mean(state1[:,st:ed,0],axis=1)),np.append(np.mean(state0[:,st:ed,1],axis=1),np.mean(state1[:,st:ed,1],axis=1))))\n",
    "gmm_Y=GaussianMixture(n_components=2,covariance_type='full').fit(Y)\n",
    "bitstring=np.split(gmm_Y.predict(Y),2)\n",
    "print('P(0|0)=',len(bitstring[0][bitstring[0]==0])/len(bitstring[0]))\n",
    "print('P(1|1)=',len(bitstring[1][bitstring[1]==1])/len(bitstring[1]))\n",
    "X=list(zip(np.mean(state0[:,st:ed,0],axis=1),np.mean(state0[:,st:ed,1],axis=1)))\n",
    "gmm_Y_1=GaussianMixture(n_components=1,covariance_type='spherical').fit(X)\n",
    "X=list(zip(np.mean(state1[:,st:ed,0],axis=1),np.mean(state1[:,st:ed,1],axis=1)))\n",
    "gmm_Y_2=GaussianMixture(n_components=1,covariance_type='spherical').fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " HiddenLayer1 (Dense)        (None, 8)                 24        \n",
      "                                                                 \n",
      " HiddenLayer2 (Dense)        (None, 4)                 36        \n",
      "                                                                 \n",
      " OuputLayer (Dense)          (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 65 (260.00 Byte)\n",
      "Trainable params: 65 (260.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def arch():\n",
    "    model=Sequential()\n",
    "    model.add(Input(shape=(2), name='Input Layer'))\n",
    "    model.add(Dense(8,activation='relu', name='HiddenLayer1'))\n",
    "    model.add(Dense(4,activation='relu', name='HiddenLayer2'))\n",
    "    model.add(Dense(1,activation='sigmoid', name='OuputLayer'))\n",
    "    return model\n",
    "arch().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bceloss(y_true, y_pred):\n",
    "    # Clip the prediction value to prevent log(0) error\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # Calculate the weighted binary cross entropy loss\n",
    "    loss = -(0.1 * (y_true * K.log(y_pred)) + 0.9*((1 - y_true) * K.log(1 - y_pred)))\n",
    "    return K.mean(loss, axis=-1)\n",
    "\n",
    "model=arch()\n",
    "model.summary\n",
    "opt=SGD( learning_rate=0.01, momentum=0.9)\n",
    "model.compile(loss=binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model_path=\"../Model/NN2/{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " 51/105 [=============>................] - ETA: 0s - loss: 0.6340 - accuracy: 0.6373  \n",
      "Epoch 1: val_accuracy improved from -inf to 0.95298, saving model to ../Model/NN2/01-0.9530.h5\n",
      "105/105 [==============================] - 1s 3ms/step - loss: 0.5413 - accuracy: 0.7937 - val_loss: 0.3530 - val_accuracy: 0.9530\n",
      "Epoch 2/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.2923 - accuracy: 0.9531"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/homes/n/nrvora/.conda/envs/readout/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_accuracy improved from 0.95298 to 0.95417, saving model to ../Model/NN2/02-0.9542.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.2646 - accuracy: 0.9531 - val_loss: 0.1939 - val_accuracy: 0.9542\n",
      "Epoch 3/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1792 - accuracy: 0.9582\n",
      "Epoch 3: val_accuracy improved from 0.95417 to 0.95536, saving model to ../Model/NN2/03-0.9554.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9530 - val_loss: 0.1650 - val_accuracy: 0.9554\n",
      "Epoch 4/100\n",
      " 60/105 [================>.............] - ETA: 0s - loss: 0.1687 - accuracy: 0.9560\n",
      "Epoch 4: val_accuracy improved from 0.95536 to 0.95714, saving model to ../Model/NN2/04-0.9571.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1685 - accuracy: 0.9555 - val_loss: 0.1573 - val_accuracy: 0.9571\n",
      "Epoch 5/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1580 - accuracy: 0.9574\n",
      "Epoch 5: val_accuracy did not improve from 0.95714\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1628 - accuracy: 0.9558 - val_loss: 0.1545 - val_accuracy: 0.9571\n",
      "Epoch 6/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1602 - accuracy: 0.9577\n",
      "Epoch 6: val_accuracy improved from 0.95714 to 0.95893, saving model to ../Model/NN2/06-0.9589.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1593 - accuracy: 0.9567 - val_loss: 0.1521 - val_accuracy: 0.9589\n",
      "Epoch 7/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1530 - accuracy: 0.9601\n",
      "Epoch 7: val_accuracy improved from 0.95893 to 0.95952, saving model to ../Model/NN2/07-0.9595.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1569 - accuracy: 0.9579 - val_loss: 0.1505 - val_accuracy: 0.9595\n",
      "Epoch 8/100\n",
      " 65/105 [=================>............] - ETA: 0s - loss: 0.1547 - accuracy: 0.9577\n",
      "Epoch 8: val_accuracy did not improve from 0.95952\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9594 - val_loss: 0.1491 - val_accuracy: 0.9589\n",
      "Epoch 9/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1491 - accuracy: 0.9609\n",
      "Epoch 9: val_accuracy improved from 0.95952 to 0.96012, saving model to ../Model/NN2/09-0.9601.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9601 - val_loss: 0.1481 - val_accuracy: 0.9601\n",
      "Epoch 10/100\n",
      " 57/105 [===============>..............] - ETA: 0s - loss: 0.1478 - accuracy: 0.9633\n",
      "Epoch 10: val_accuracy did not improve from 0.96012\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1519 - accuracy: 0.9607 - val_loss: 0.1476 - val_accuracy: 0.9595\n",
      "Epoch 11/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1504 - accuracy: 0.9616\n",
      "Epoch 11: val_accuracy improved from 0.96012 to 0.96190, saving model to ../Model/NN2/11-0.9619.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9615 - val_loss: 0.1463 - val_accuracy: 0.9619\n",
      "Epoch 12/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1554 - accuracy: 0.9599\n",
      "Epoch 12: val_accuracy did not improve from 0.96190\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1493 - accuracy: 0.9613 - val_loss: 0.1454 - val_accuracy: 0.9619\n",
      "Epoch 13/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1493 - accuracy: 0.9619\n",
      "Epoch 13: val_accuracy did not improve from 0.96190\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1485 - accuracy: 0.9618 - val_loss: 0.1449 - val_accuracy: 0.9619\n",
      "Epoch 14/100\n",
      " 58/105 [===============>..............] - ETA: 0s - loss: 0.1436 - accuracy: 0.9636\n",
      "Epoch 14: val_accuracy improved from 0.96190 to 0.96250, saving model to ../Model/NN2/14-0.9625.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1474 - accuracy: 0.9619 - val_loss: 0.1447 - val_accuracy: 0.9625\n",
      "Epoch 15/100\n",
      " 68/105 [==================>...........] - ETA: 0s - loss: 0.1504 - accuracy: 0.9607\n",
      "Epoch 15: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9618 - val_loss: 0.1440 - val_accuracy: 0.9607\n",
      "Epoch 16/100\n",
      " 67/105 [==================>...........] - ETA: 0s - loss: 0.1396 - accuracy: 0.9632\n",
      "Epoch 16: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1462 - accuracy: 0.9615 - val_loss: 0.1440 - val_accuracy: 0.9619\n",
      "Epoch 17/100\n",
      " 65/105 [=================>............] - ETA: 0s - loss: 0.1480 - accuracy: 0.9591\n",
      "Epoch 17: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9612 - val_loss: 0.1433 - val_accuracy: 0.9625\n",
      "Epoch 18/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1372 - accuracy: 0.9634\n",
      "Epoch 18: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1453 - accuracy: 0.9613 - val_loss: 0.1431 - val_accuracy: 0.9625\n",
      "Epoch 19/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1457 - accuracy: 0.9617\n",
      "Epoch 19: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1448 - accuracy: 0.9610 - val_loss: 0.1430 - val_accuracy: 0.9625\n",
      "Epoch 20/100\n",
      " 66/105 [=================>............] - ETA: 0s - loss: 0.1478 - accuracy: 0.9607\n",
      "Epoch 20: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1445 - accuracy: 0.9609 - val_loss: 0.1428 - val_accuracy: 0.9625\n",
      "Epoch 21/100\n",
      " 68/105 [==================>...........] - ETA: 0s - loss: 0.1495 - accuracy: 0.9591\n",
      "Epoch 21: val_accuracy did not improve from 0.96250\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9609 - val_loss: 0.1424 - val_accuracy: 0.9625\n",
      "Epoch 22/100\n",
      " 66/105 [=================>............] - ETA: 0s - loss: 0.1453 - accuracy: 0.9605\n",
      "Epoch 22: val_accuracy improved from 0.96250 to 0.96310, saving model to ../Model/NN2/22-0.9631.h5\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1442 - accuracy: 0.9607 - val_loss: 0.1431 - val_accuracy: 0.9631\n",
      "Epoch 23/100\n",
      " 68/105 [==================>...........] - ETA: 0s - loss: 0.1535 - accuracy: 0.9593\n",
      "Epoch 23: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1438 - accuracy: 0.9607 - val_loss: 0.1422 - val_accuracy: 0.9625\n",
      "Epoch 24/100\n",
      " 67/105 [==================>...........] - ETA: 0s - loss: 0.1435 - accuracy: 0.9592\n",
      "Epoch 24: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9609 - val_loss: 0.1422 - val_accuracy: 0.9625\n",
      "Epoch 25/100\n",
      " 66/105 [=================>............] - ETA: 0s - loss: 0.1422 - accuracy: 0.9621\n",
      "Epoch 25: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9610 - val_loss: 0.1423 - val_accuracy: 0.9625\n",
      "Epoch 26/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1480 - accuracy: 0.9593\n",
      "Epoch 26: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9604 - val_loss: 0.1423 - val_accuracy: 0.9625\n",
      "Epoch 27/100\n",
      " 66/105 [=================>............] - ETA: 0s - loss: 0.1409 - accuracy: 0.9616\n",
      "Epoch 27: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1434 - accuracy: 0.9606 - val_loss: 0.1423 - val_accuracy: 0.9625\n",
      "Epoch 28/100\n",
      " 66/105 [=================>............] - ETA: 0s - loss: 0.1406 - accuracy: 0.9626\n",
      "Epoch 28: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9606 - val_loss: 0.1422 - val_accuracy: 0.9625\n",
      "Epoch 29/100\n",
      " 94/105 [=========================>....] - ETA: 0s - loss: 0.1431 - accuracy: 0.9608\n",
      "Epoch 29: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9609 - val_loss: 0.1421 - val_accuracy: 0.9625\n",
      "Epoch 30/100\n",
      " 53/105 [==============>...............] - ETA: 0s - loss: 0.1381 - accuracy: 0.9614\n",
      "Epoch 30: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9604 - val_loss: 0.1419 - val_accuracy: 0.9625\n",
      "Epoch 31/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1455 - accuracy: 0.9600\n",
      "Epoch 31: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1431 - accuracy: 0.9609 - val_loss: 0.1421 - val_accuracy: 0.9625\n",
      "Epoch 32/100\n",
      "104/105 [============================>.] - ETA: 0s - loss: 0.1433 - accuracy: 0.9603\n",
      "Epoch 32: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1430 - accuracy: 0.9604 - val_loss: 0.1422 - val_accuracy: 0.9625\n",
      "Epoch 33/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1407 - accuracy: 0.9611\n",
      "Epoch 33: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9610 - val_loss: 0.1418 - val_accuracy: 0.9625\n",
      "Epoch 34/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1411 - accuracy: 0.9619\n",
      "Epoch 34: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9609 - val_loss: 0.1421 - val_accuracy: 0.9625\n",
      "Epoch 35/100\n",
      " 56/105 [===============>..............] - ETA: 0s - loss: 0.1455 - accuracy: 0.9595\n",
      "Epoch 35: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9604 - val_loss: 0.1426 - val_accuracy: 0.9625\n",
      "Epoch 36/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1435 - accuracy: 0.9602\n",
      "Epoch 36: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9607 - val_loss: 0.1427 - val_accuracy: 0.9625\n",
      "Epoch 37/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1422 - accuracy: 0.9608\n",
      "Epoch 37: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9607 - val_loss: 0.1427 - val_accuracy: 0.9625\n",
      "Epoch 38/100\n",
      " 60/105 [================>.............] - ETA: 0s - loss: 0.1349 - accuracy: 0.9633\n",
      "Epoch 38: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1429 - accuracy: 0.9607 - val_loss: 0.1419 - val_accuracy: 0.9625\n",
      "Epoch 39/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1446 - accuracy: 0.9593\n",
      "Epoch 39: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1428 - accuracy: 0.9609 - val_loss: 0.1424 - val_accuracy: 0.9625\n",
      "Epoch 40/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1395 - accuracy: 0.9618\n",
      "Epoch 40: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9612 - val_loss: 0.1417 - val_accuracy: 0.9625\n",
      "Epoch 41/100\n",
      " 60/105 [================>.............] - ETA: 0s - loss: 0.1488 - accuracy: 0.9581\n",
      "Epoch 41: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1427 - accuracy: 0.9607 - val_loss: 0.1420 - val_accuracy: 0.9625\n",
      "Epoch 42/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1408 - accuracy: 0.9594\n",
      "Epoch 42: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9609 - val_loss: 0.1416 - val_accuracy: 0.9625\n",
      "Epoch 43/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1487 - accuracy: 0.9599\n",
      "Epoch 43: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9609 - val_loss: 0.1418 - val_accuracy: 0.9625\n",
      "Epoch 44/100\n",
      " 58/105 [===============>..............] - ETA: 0s - loss: 0.1375 - accuracy: 0.9631\n",
      "Epoch 44: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9612 - val_loss: 0.1427 - val_accuracy: 0.9625\n",
      "Epoch 45/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1512 - accuracy: 0.9572\n",
      "Epoch 45: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9609 - val_loss: 0.1421 - val_accuracy: 0.9619\n",
      "Epoch 46/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1511 - accuracy: 0.9565\n",
      "Epoch 46: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9609 - val_loss: 0.1429 - val_accuracy: 0.9625\n",
      "Epoch 47/100\n",
      " 60/105 [================>.............] - ETA: 0s - loss: 0.1413 - accuracy: 0.9633\n",
      "Epoch 47: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9616 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
      "Epoch 48/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1460 - accuracy: 0.9599\n",
      "Epoch 48: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1426 - accuracy: 0.9609 - val_loss: 0.1422 - val_accuracy: 0.9619\n",
      "Epoch 49/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1333 - accuracy: 0.9633\n",
      "Epoch 49: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9613 - val_loss: 0.1425 - val_accuracy: 0.9619\n",
      "Epoch 50/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1507 - accuracy: 0.9594\n",
      "Epoch 50: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9607 - val_loss: 0.1415 - val_accuracy: 0.9619\n",
      "Epoch 51/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1486 - accuracy: 0.9586\n",
      "Epoch 51: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9610 - val_loss: 0.1424 - val_accuracy: 0.9625\n",
      "Epoch 52/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1487 - accuracy: 0.9583\n",
      "Epoch 52: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9610 - val_loss: 0.1415 - val_accuracy: 0.9619\n",
      "Epoch 53/100\n",
      " 60/105 [================>.............] - ETA: 0s - loss: 0.1328 - accuracy: 0.9630\n",
      "Epoch 53: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9610 - val_loss: 0.1427 - val_accuracy: 0.9619\n",
      "Epoch 54/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1460 - accuracy: 0.9595\n",
      "Epoch 54: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1423 - accuracy: 0.9612 - val_loss: 0.1422 - val_accuracy: 0.9619\n",
      "Epoch 55/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1449 - accuracy: 0.9612\n",
      "Epoch 55: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1425 - accuracy: 0.9609 - val_loss: 0.1416 - val_accuracy: 0.9619\n",
      "Epoch 56/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1404 - accuracy: 0.9623\n",
      "Epoch 56: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9613 - val_loss: 0.1415 - val_accuracy: 0.9619\n",
      "Epoch 57/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1422 - accuracy: 0.9617\n",
      "Epoch 57: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1423 - accuracy: 0.9609 - val_loss: 0.1418 - val_accuracy: 0.9619\n",
      "Epoch 58/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1294 - accuracy: 0.9650\n",
      "Epoch 58: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9612 - val_loss: 0.1424 - val_accuracy: 0.9619\n",
      "Epoch 59/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1532 - accuracy: 0.9568\n",
      "Epoch 59: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9609 - val_loss: 0.1421 - val_accuracy: 0.9619\n",
      "Epoch 60/100\n",
      " 59/105 [===============>..............] - ETA: 0s - loss: 0.1422 - accuracy: 0.9603\n",
      "Epoch 60: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9610 - val_loss: 0.1418 - val_accuracy: 0.9619\n",
      "Epoch 61/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1445 - accuracy: 0.9597\n",
      "Epoch 61: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9609 - val_loss: 0.1430 - val_accuracy: 0.9619\n",
      "Epoch 62/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1340 - accuracy: 0.9645\n",
      "Epoch 62: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9613 - val_loss: 0.1425 - val_accuracy: 0.9619\n",
      "Epoch 63/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1429 - accuracy: 0.9598\n",
      "Epoch 63: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9607 - val_loss: 0.1419 - val_accuracy: 0.9619\n",
      "Epoch 64/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1365 - accuracy: 0.9626\n",
      "Epoch 64: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9607 - val_loss: 0.1422 - val_accuracy: 0.9619\n",
      "Epoch 65/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1474 - accuracy: 0.9582\n",
      "Epoch 65: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9609 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
      "Epoch 66/100\n",
      " 60/105 [================>.............] - ETA: 0s - loss: 0.1443 - accuracy: 0.9602\n",
      "Epoch 66: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9610 - val_loss: 0.1425 - val_accuracy: 0.9619\n",
      "Epoch 67/100\n",
      " 59/105 [===============>..............] - ETA: 0s - loss: 0.1363 - accuracy: 0.9637\n",
      "Epoch 67: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9615 - val_loss: 0.1425 - val_accuracy: 0.9619\n",
      "Epoch 68/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1421 - accuracy: 0.9622\n",
      "Epoch 68: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.9610 - val_loss: 0.1416 - val_accuracy: 0.9619\n",
      "Epoch 69/100\n",
      " 58/105 [===============>..............] - ETA: 0s - loss: 0.1423 - accuracy: 0.9599\n",
      "Epoch 69: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9612 - val_loss: 0.1419 - val_accuracy: 0.9619\n",
      "Epoch 70/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1348 - accuracy: 0.9648\n",
      "Epoch 70: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1422 - accuracy: 0.9613 - val_loss: 0.1426 - val_accuracy: 0.9619\n",
      "Epoch 71/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1432 - accuracy: 0.9603\n",
      "Epoch 71: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9610 - val_loss: 0.1428 - val_accuracy: 0.9619\n",
      "Epoch 72/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1428 - accuracy: 0.9600\n",
      "Epoch 72: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9610 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
      "Epoch 73/100\n",
      " 65/105 [=================>............] - ETA: 0s - loss: 0.1413 - accuracy: 0.9620\n",
      "Epoch 73: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.9616 - val_loss: 0.1415 - val_accuracy: 0.9619\n",
      "Epoch 74/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1507 - accuracy: 0.9588\n",
      "Epoch 74: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9610 - val_loss: 0.1415 - val_accuracy: 0.9619\n",
      "Epoch 75/100\n",
      " 67/105 [==================>...........] - ETA: 0s - loss: 0.1489 - accuracy: 0.9604\n",
      "Epoch 75: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9612 - val_loss: 0.1420 - val_accuracy: 0.9619\n",
      "Epoch 76/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1414 - accuracy: 0.9608\n",
      "Epoch 76: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1420 - accuracy: 0.9616 - val_loss: 0.1421 - val_accuracy: 0.9619\n",
      "Epoch 77/100\n",
      " 61/105 [================>.............] - ETA: 0s - loss: 0.1454 - accuracy: 0.9593\n",
      "Epoch 77: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9609 - val_loss: 0.1416 - val_accuracy: 0.9619\n",
      "Epoch 78/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1389 - accuracy: 0.9614\n",
      "Epoch 78: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9615 - val_loss: 0.1418 - val_accuracy: 0.9619\n",
      "Epoch 79/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1415 - accuracy: 0.9621\n",
      "Epoch 79: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9618 - val_loss: 0.1422 - val_accuracy: 0.9619\n",
      "Epoch 80/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1355 - accuracy: 0.9627\n",
      "Epoch 80: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9609 - val_loss: 0.1426 - val_accuracy: 0.9619\n",
      "Epoch 81/100\n",
      " 62/105 [================>.............] - ETA: 0s - loss: 0.1329 - accuracy: 0.9647\n",
      "Epoch 81: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9618 - val_loss: 0.1423 - val_accuracy: 0.9619\n",
      "Epoch 82/100\n",
      " 64/105 [=================>............] - ETA: 0s - loss: 0.1366 - accuracy: 0.9634\n",
      "Epoch 82: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9610 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
      "Epoch 83/100\n",
      " 67/105 [==================>...........] - ETA: 0s - loss: 0.1455 - accuracy: 0.9594\n",
      "Epoch 83: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9612 - val_loss: 0.1421 - val_accuracy: 0.9613\n",
      "Epoch 84/100\n",
      " 67/105 [==================>...........] - ETA: 0s - loss: 0.1419 - accuracy: 0.9608\n",
      "Epoch 84: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9613 - val_loss: 0.1420 - val_accuracy: 0.9613\n",
      "Epoch 85/100\n",
      " 67/105 [==================>...........] - ETA: 0s - loss: 0.1436 - accuracy: 0.9613\n",
      "Epoch 85: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9615 - val_loss: 0.1424 - val_accuracy: 0.9613\n",
      "Epoch 86/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1381 - accuracy: 0.9635\n",
      "Epoch 86: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9610 - val_loss: 0.1420 - val_accuracy: 0.9619\n",
      "Epoch 87/100\n",
      " 63/105 [=================>............] - ETA: 0s - loss: 0.1316 - accuracy: 0.9648\n",
      "Epoch 87: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9616 - val_loss: 0.1418 - val_accuracy: 0.9619\n",
      "Epoch 88/100\n",
      " 58/105 [===============>..............] - ETA: 0s - loss: 0.1502 - accuracy: 0.9585\n",
      "Epoch 88: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9610 - val_loss: 0.1416 - val_accuracy: 0.9613\n",
      "Epoch 89/100\n",
      " 54/105 [==============>...............] - ETA: 0s - loss: 0.1305 - accuracy: 0.9644\n",
      "Epoch 89: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9616 - val_loss: 0.1421 - val_accuracy: 0.9613\n",
      "Epoch 90/100\n",
      "103/105 [============================>.] - ETA: 0s - loss: 0.1419 - accuracy: 0.9610\n",
      "Epoch 90: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9612 - val_loss: 0.1420 - val_accuracy: 0.9613\n",
      "Epoch 91/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.1423 - accuracy: 0.9616\n",
      "Epoch 91: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9618 - val_loss: 0.1423 - val_accuracy: 0.9613\n",
      "Epoch 92/100\n",
      "102/105 [============================>.] - ETA: 0s - loss: 0.1417 - accuracy: 0.9611\n",
      "Epoch 92: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9610 - val_loss: 0.1420 - val_accuracy: 0.9613\n",
      "Epoch 93/100\n",
      "101/105 [===========================>..] - ETA: 0s - loss: 0.1419 - accuracy: 0.9616\n",
      "Epoch 93: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9615 - val_loss: 0.1420 - val_accuracy: 0.9613\n",
      "Epoch 94/100\n",
      "105/105 [==============================] - ETA: 0s - loss: 0.1417 - accuracy: 0.9610\n",
      "Epoch 94: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9610 - val_loss: 0.1414 - val_accuracy: 0.9619\n",
      "Epoch 95/100\n",
      " 58/105 [===============>..............] - ETA: 0s - loss: 0.1370 - accuracy: 0.9642\n",
      "Epoch 95: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9618 - val_loss: 0.1417 - val_accuracy: 0.9619\n",
      "Epoch 96/100\n",
      " 56/105 [===============>..............] - ETA: 0s - loss: 0.1425 - accuracy: 0.9607\n",
      "Epoch 96: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9609 - val_loss: 0.1419 - val_accuracy: 0.9613\n",
      "Epoch 97/100\n",
      " 56/105 [===============>..............] - ETA: 0s - loss: 0.1491 - accuracy: 0.9601\n",
      "Epoch 97: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1418 - accuracy: 0.9613 - val_loss: 0.1423 - val_accuracy: 0.9613\n",
      "Epoch 98/100\n",
      " 56/105 [===============>..............] - ETA: 0s - loss: 0.1363 - accuracy: 0.9615\n",
      "Epoch 98: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9615 - val_loss: 0.1423 - val_accuracy: 0.9613\n",
      "Epoch 99/100\n",
      " 57/105 [===============>..............] - ETA: 0s - loss: 0.1464 - accuracy: 0.9581\n",
      "Epoch 99: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1419 - accuracy: 0.9612 - val_loss: 0.1423 - val_accuracy: 0.9613\n",
      "Epoch 100/100\n",
      " 55/105 [==============>...............] - ETA: 0s - loss: 0.1447 - accuracy: 0.9608\n",
      "Epoch 100: val_accuracy did not improve from 0.96310\n",
      "105/105 [==============================] - 0s 2ms/step - loss: 0.1417 - accuracy: 0.9615 - val_loss: 0.1427 - val_accuracy: 0.9613\n"
     ]
    }
   ],
   "source": [
    "H=model.fit(x_train,y_train,\n",
    "          validation_split=0.2,\n",
    "          epochs=100,batch_size=64,\n",
    "          callbacks=callbacks_list,\n",
    "          verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 0s 510us/step\n",
      "1767 1713\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "count1=0\n",
    "y_pred=model.predict(x_test)\n",
    "for i in range(y_pred.shape[0]):\n",
    "    if y_test[i]==1:\n",
    "        count+=1\n",
    "        if y_pred[i]>=0.5:\n",
    "            count1+=1\n",
    "print(count,count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../x_text.npy',x_test)\n",
    "np.save('../y_test.npy',y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.15%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GaussianDiscriminantAnalysis:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.class_priors = np.array([np.mean(y == c) for c in self.classes])\n",
    "\n",
    "        self.means = []\n",
    "        self.cov_matrices = []\n",
    "        for c in self.classes:\n",
    "            X_c = X[y == c]\n",
    "            mean_c = np.mean(X_c, axis=0)\n",
    "            cov_matrix_c = np.cov(X_c, rowvar=False)\n",
    "            self.means.append(mean_c)\n",
    "            self.cov_matrices.append(cov_matrix_c)\n",
    "\n",
    "    def predict(self, X):\n",
    "        posteriors = []\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_prior = self.class_priors[int(c)]\n",
    "            mean = self.means[int(c)]\n",
    "            cov_matrix = self.cov_matrices[int(c)]\n",
    "            mvn = multivariate_normal(mean=mean, cov=cov_matrix)\n",
    "            posterior = class_prior * mvn.pdf(X)\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        posteriors = np.array(posteriors).T\n",
    "        predicted_labels = np.argmax(posteriors, axis=1)\n",
    "\n",
    "        return predicted_labels\n",
    "\n",
    "\n",
    "\n",
    "# Initialize and train GDA\n",
    "gda = GaussianDiscriminantAnalysis()\n",
    "gda.fit(x_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gda.predict(x_train)\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(y_pred == y_train)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: -32291.412, Max: 28569.142\n",
      "Min: 0.254, Max: 0.718\n"
     ]
    }
   ],
   "source": [
    "min,max= -(2**16),(2**16)\n",
    "print('Min: %.3f, Max: %.3f' % (x.min(), x.max()))\n",
    "x= (x-min)/(max-min)\n",
    "print('Min: %.3f, Max: %.3f' % (x.min(), x.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8400, 200, 20)\n",
      "4233.0\n"
     ]
    }
   ],
   "source": [
    "st,ed=700,2700\n",
    "x_train, x_test, y_train, y_test = train_test_split(x[:,st:ed,:], y, test_size=0.30, random_state=45)\n",
    "x_train=x_train.reshape((x_train.shape[0],200,20))\n",
    "x_test=x_test.reshape((x_test.shape[0],200,20))\n",
    "print(x_train.shape)\n",
    "print(y_train.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn():\n",
    "    model=Sequential()\n",
    "    model.add(Input(shape=(200,20)))\n",
    "    model.add(layers.LSTM(units=16, return_sequences=True))\n",
    "    model.add(layers.LSTM(units=8, return_sequences=True))\n",
    "    model.add(layers.LSTM(units=4, return_sequences=False))\n",
    "#     model.add(layers.Dense(4,activation='relu'))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_29 (LSTM)              (None, 200, 16)           2368      \n",
      "                                                                 \n",
      " lstm_30 (LSTM)              (None, 200, 8)            800       \n",
      "                                                                 \n",
      " lstm_31 (LSTM)              (None, 4)                 208       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3381 (13.21 KB)\n",
      "Trainable params: 3381 (13.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=rnn()\n",
    "model.summary()\n",
    "opt=SGD( learning_rate=0.001, momentum=0.9)\n",
    "model.compile(loss=binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model_path=\"../Model/RNN_new/{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-12 14:58:39.275865: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:425] Loaded runtime CuDNN library: 8.3.2 but source was compiled with: 8.6.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2023-10-12 14:58:39.276790: W tensorflow/core/framework/op_kernel.cc:1828] OP_REQUIRES failed at cudnn_rnn_ops.cc:1762 : UNKNOWN: Fail to find the dnn implementation.\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_11/lstm_29/PartitionedCall]] [Op:__inference_train_function_29808]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# with tf.device(\"/gpu:0\"):\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m H\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/readout/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/readout/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFail to find the dnn implementation.\n\t [[{{node CudnnRNN}}]]\n\t [[sequential_11/lstm_29/PartitionedCall]] [Op:__inference_train_function_29808]"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    H=model.fit(x_train,y_train,\n",
    "            validation_data=(x_test, y_test),\n",
    "            epochs=200,batch_size=32,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HLS4ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cat: /etc/lsb-release: No such file or directory\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'cat /etc/lsb-release\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcat /etc/lsb-release\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.12.0/lib/python3.9/site-packages/IPython/core/interactiveshell.py:2478\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2477\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2478\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2480\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2481\u001b[0m \u001b[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2482\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.12.0/lib/python3.9/site-packages/IPython/core/magics/script.py:153\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.12.0/lib/python3.9/site-packages/IPython/core/magics/script.py:305\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    304\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'cat /etc/lsb-release\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat /etc/lsb-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/global/homes/n/nrvora/.local/perlmutter/tensorflow2.12.0/bin:/global/common/software/nersc/pm-2022q4/sw/tensorflow/2.12.0/bin:/global/common/software/nersc/pm-2022q4/sw/evp-patch/bin:/global/homes/n/nrvora/.conda/envs/readout/bin:/global/common/software/nersc/pe/conda/23.9.0/Miniconda3-py311_23.5.2-0/condabin:/global/u1/n/nrvora/.vscode-server/bin/f1b07bd25dfad64b0167beb15359ae573aecd2cc/bin/remote-cli:/opt/nersc/pe/bin:/global/common/software/nersc/bin:/global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Systems/2022.2.1:/global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Systems/2022.2.1/bin:/global/common/software/nersc/pm-2021q4/easybuild/software/Nsight-Compute/2022.1.1:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/compute-sanitizer:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/bin:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/cuda/11.7/libnvvp:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/profilers/Nsight_Compute:/opt/nvidia/hpc_sdk/Linux_x86_64/22.7/profilers/Nsight_Systems/bin:/opt/cray/pe/perftools/23.03.0/bin:/opt/cray/pe/papi/7.0.0.1/bin:/opt/cray/pe/gcc/11.2.0/bin:/opt/cray/pe/craype/2.7.20/bin:/opt/cray/pe/mpich/8.1.25/ofi/gnu/9.1/bin:/opt/cray/pe/mpich/8.1.25/bin:/opt/cray/libfabric/1.15.2.0/bin:/usr/local/bin:/usr/bin:/bin:/usr/lib/mit/bin:/opt/cray/pe/bin'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['PATH'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "readout-jupyter",
   "language": "python",
   "name": "readout-jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
