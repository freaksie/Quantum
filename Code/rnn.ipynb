{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 18:14:42.569119: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 18:14:42.692183: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-31 18:14:42.720569: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-07-31 18:14:43.170984: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:\n",
      "2023-07-31 18:14:43.171044: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.8/lib64:\n",
      "2023-07-31 18:14:43.171049: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import layers,Input\n",
    "from keras.models import  Sequential,load_model\n",
    "from keras.optimizers import SGD\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1L2\n",
    "import keras.backend as K\n",
    "import hls4ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 2000)\n"
     ]
    }
   ],
   "source": [
    "xTrain=np.load(\"../Data/julyData/trainData.npy\")\n",
    "yTrain=np.load(\"../Data/julyData/trainTarget.npy\")\n",
    "xTest=np.load('../Data/julyData/testData.npy')\n",
    "yTest=np.load(\"../Data/julyData/testTarget.npy\")\n",
    "\n",
    "X_train=np.reshape(xTrain,(xTrain.shape[0],20,100))\n",
    "X_test=np.reshape(xTest,(xTest.shape[0],20,100))\n",
    "\n",
    "xTrain_r=np.reshape(xTrain,(xTrain.shape[0],xTrain.shape[1]))\n",
    "yTrain=np.reshape(yTrain,(yTrain.shape[0]))\n",
    "xTest_r=np.reshape(xTest,(xTest.shape[0],xTest.shape[1]))\n",
    "yTest=np.reshape(yTest,(yTest.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "print(xTrain.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.37314513 7.26858534 6.27678229 5.14826494 4.95262119 4.8444645\n",
      " 4.75130361 4.68458792 4.61031313 4.58430339 4.55782553 4.54001239\n",
      " 4.50234835 4.48828114 4.42802519 4.41803633 4.39750343 4.37832726\n",
      " 4.35236683 4.34490985 4.30726738 4.27218349 4.26925878 4.25913927\n",
      " 4.23343171 4.21829447 4.20959671 4.20823033 4.18231654 4.16091432\n",
      " 4.14612117 4.13240374 4.12055162 4.11816321 4.11055576 4.09350266\n",
      " 4.08515321 4.06852059 4.04185867 4.03600707 4.02226238 3.98570262\n",
      " 3.97898116 3.97095478 3.95014959 3.93359959 3.93095068 3.92719986\n",
      " 3.91112038 3.89741348 3.88378391 3.87126103 3.85236418 3.84463824\n",
      " 3.82931067 3.82161276 3.79946108 3.79086359 3.77808843 3.76439314\n",
      " 3.75628178 3.74143344 3.73046817 3.71326198 3.70695289 3.7021813\n",
      " 3.6904227  3.68561329 3.6633561  3.65153596 3.62176593 3.61476363\n",
      " 3.6071491  3.59576933 3.58563109 3.57460738 3.5580075  3.53806119\n",
      " 3.53423157 3.51805909 3.51011668 3.50201002 3.49864692 3.48815357\n",
      " 3.46867816 3.44627638 3.44102582 3.43040969 3.41684364 3.41037971\n",
      " 3.39164291 3.37913166 3.36921478 3.36161384 3.35501232 3.33034507\n",
      " 3.31261804 3.3026401  3.28583299 3.2588161 ]\n",
      "[0.00368427 0.00363202 0.00313643 0.00257252 0.00247476 0.00242072\n",
      " 0.00237417 0.00234083 0.00230372 0.00229072 0.00227749 0.00226859\n",
      " 0.00224977 0.00224274 0.00221263 0.00220764 0.00219738 0.0021878\n",
      " 0.00217482 0.0021711  0.00215229 0.00213476 0.0021333  0.00212824\n",
      " 0.00211539 0.00210783 0.00210348 0.0021028  0.00208985 0.00207916\n",
      " 0.00207176 0.00206491 0.00205899 0.00205779 0.00205399 0.00204547\n",
      " 0.0020413  0.00203299 0.00201967 0.00201674 0.00200987 0.00199161\n",
      " 0.00198825 0.00198424 0.00197384 0.00196557 0.00196425 0.00196237\n",
      " 0.00195434 0.00194749 0.00194068 0.00193442 0.00192498 0.00192112\n",
      " 0.00191346 0.00190961 0.00189854 0.00189425 0.00188786 0.00188102\n",
      " 0.00187697 0.00186955 0.00186407 0.00185547 0.00185232 0.00184993\n",
      " 0.00184406 0.00184165 0.00183053 0.00182463 0.00180975 0.00180625\n",
      " 0.00180245 0.00179676 0.0017917  0.00178619 0.00177789 0.00176792\n",
      " 0.00176601 0.00175793 0.00175396 0.00174991 0.00174823 0.00174299\n",
      " 0.00173326 0.00172206 0.00171944 0.00171413 0.00170735 0.00170412\n",
      " 0.00169476 0.00168851 0.00168355 0.00167976 0.00167646 0.00166413\n",
      " 0.00165527 0.00165029 0.00164189 0.00162839]\n",
      "[0.00368427 0.00731629 0.01045272 0.01302524 0.01550001 0.01792072\n",
      " 0.02029489 0.02263572 0.02493944 0.02723016 0.02950764 0.03177623\n",
      " 0.034026   0.03626874 0.03848137 0.040689   0.04288638 0.04507418\n",
      " 0.047249   0.0494201  0.05157238 0.05370714 0.05584044 0.05796868\n",
      " 0.06008407 0.0621919  0.06429538 0.06639818 0.06848803 0.07056719\n",
      " 0.07263895 0.07470386 0.07676285 0.07882065 0.08087464 0.08292011\n",
      " 0.08496141 0.0869944  0.08901407 0.09103081 0.09304068 0.09503229\n",
      " 0.09702054 0.09900477 0.10097861 0.10294418 0.10490843 0.1068708\n",
      " 0.10882514 0.11077263 0.11271331 0.11464773 0.11657271 0.11849383\n",
      " 0.12040728 0.1223169  0.12421544 0.12610969 0.12799755 0.12987857\n",
      " 0.13175554 0.13362508 0.13548915 0.13734462 0.13919694 0.14104688\n",
      " 0.14289093 0.14473259 0.14656312 0.14838775 0.1501975  0.15200375\n",
      " 0.1538062  0.15560296 0.15739466 0.15918084 0.16095873 0.16272666\n",
      " 0.16449267 0.1662506  0.16800456 0.16975447 0.1715027  0.17324569\n",
      " 0.17497894 0.17670101 0.17842044 0.18013458 0.18184193 0.18354605\n",
      " 0.18524082 0.18692933 0.18861288 0.19029264 0.19196909 0.19363323\n",
      " 0.1952885  0.19693879 0.19858068 0.20020907]\n",
      "X shape: (1600, 100)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(xTrain_r)\n",
    "X_test = sc.transform(xTest_r)\n",
    "pca = PCA(n_components = 100)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "print (pca.explained_variance_)\n",
    "print (pca.explained_variance_ratio_)\n",
    "print (pca.explained_variance_ratio_.cumsum())\n",
    "X_test = pca.transform(X_test)\n",
    "print(\"X shape: {}\".format(X_train.shape))\n",
    "del xTest,xTrain,xTest_r,xTrain_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 10, 10) (1600,)\n"
     ]
    }
   ],
   "source": [
    "X_train=np.reshape(X_train,(X_train.shape[0],10,10))\n",
    "yTrain=np.reshape(yTrain,(yTrain.shape[0]))\n",
    "X_test=np.reshape(X_test,(X_test.shape[0],10,10))\n",
    "yTest=np.reshape(yTest,(yTest.shape[0]))\n",
    "print(X_train.shape, yTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch():\n",
    "    model=Sequential()\n",
    "    model.add(Input(shape=(10,10)))\n",
    "    # model.add(layers.CuDNNLSTM(units=256, return_sequences=True,kernel_regularizer=L1L2(0.0001)))\n",
    "    model.add(layers.LSTM(units=64, return_sequences=True, kernel_regularizer=L1L2(0.00001)))\n",
    "    model.add(layers.LSTM(units=64, return_sequences=False,kernel_regularizer=L1L2(0.00001)))\n",
    "    model.add(layers.Dense(64,activation='selu'))\n",
    "    model.add(layers.Dropout(0.6))\n",
    "    model.add(layers.Dense(32,activation='selu'))\n",
    "    model.add(layers.Dropout(0.6))\n",
    "    model.add(layers.Dense(16,activation='selu'))\n",
    "    model.add(layers.Dropout(0.6))\n",
    "    model.add(layers.Dense(4,activation='selu'))\n",
    "    model.add(layers.Dropout(0.6))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "def arch2():\n",
    "    model=Sequential()\n",
    "    model.add(Input(shape=(None,100)))\n",
    "    model.add(layers.CuDNNLSTM(units=256,return_sequences=True,kernel_regularizer=L1L2(0.000001)))\n",
    "    model.add(layers.CuDNNLSTM(units=128,return_sequences=False,kernel_regularizer=L1L2(0.000001)))\n",
    "    model.add(layers.Dense(128,activation='selu'))\n",
    "    model.add(layers.Dense(64,activation='selu'))\n",
    "    model.add(layers.Dense(32,activation='selu'))\n",
    "    model.add(layers.Dense(16,activation='selu'))\n",
    "    model.add(layers.Dense(8,activation='selu'))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    return model\n",
    "# arch2().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "def loss(y_true, y_pred):\n",
    "\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "\n",
    "    # Calculate the binary cross entropy\n",
    "    bce =  -((y_true * K.log(y_pred)) + (1 - y_true) * K.log(1 - y_pred))\n",
    "    \n",
    "    mean_bce = K.mean(bce, axis=-1)\n",
    "\n",
    "    return mean_bce\n",
    "    \n",
    "def f1_score(y_true, y_pred):\n",
    "    y_true = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "    y_pred = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "    # clip predictions to avoid 0/0 division\n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    # calculate true positives, false positives, and false negatives\n",
    "    tp = K.sum(K.round(y_true * y_pred))\n",
    "    fp = K.sum(K.round(K.clip(y_pred - y_true, 0, 1)))\n",
    "    fn = K.sum(K.round(K.clip(y_true - y_pred, 0, 1)))\n",
    "    # calculate precision and recall\n",
    "    precision = tp / (tp + fp + K.epsilon())\n",
    "    recall = tp / (tp + fn + K.epsilon())\n",
    "    # calculate F1 score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 10, 64)            19200     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 4)                 0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 59,065\n",
      "Trainable params: 59,065\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=arch()\n",
    "model.summary()\n",
    "opt=SGD( learning_rate=0.001, momentum=0.9)\n",
    "model.compile(loss=binary_crossentropy, optimizer='adam', metrics=['accuracy'])\n",
    "model_path=\"../Model/RNN_new/{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.9501 - accuracy: 0.4987\n",
      "Epoch 1: val_accuracy improved from -inf to 0.56500, saving model to ../Model/RNN_new/01-0.5650.h5\n",
      "50/50 [==============================] - 4s 32ms/step - loss: 0.9494 - accuracy: 0.5006 - val_loss: 0.6900 - val_accuracy: 0.5650\n",
      "Epoch 2/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8625 - accuracy: 0.5344\n",
      "Epoch 2: val_accuracy improved from 0.56500 to 0.62750, saving model to ../Model/RNN_new/02-0.6275.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.8625 - accuracy: 0.5344 - val_loss: 0.6631 - val_accuracy: 0.6275\n",
      "Epoch 3/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.8411 - accuracy: 0.5213\n",
      "Epoch 3: val_accuracy improved from 0.62750 to 0.66000, saving model to ../Model/RNN_new/03-0.6600.h5\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.8411 - accuracy: 0.5213 - val_loss: 0.6567 - val_accuracy: 0.6600\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7714 - accuracy: 0.5462\n",
      "Epoch 4: val_accuracy improved from 0.66000 to 0.66500, saving model to ../Model/RNN_new/04-0.6650.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.7714 - accuracy: 0.5462 - val_loss: 0.6413 - val_accuracy: 0.6650\n",
      "Epoch 5/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7609 - accuracy: 0.5402\n",
      "Epoch 5: val_accuracy improved from 0.66500 to 0.71500, saving model to ../Model/RNN_new/05-0.7150.h5\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.7632 - accuracy: 0.5406 - val_loss: 0.6279 - val_accuracy: 0.7150\n",
      "Epoch 6/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7526 - accuracy: 0.5580\n",
      "Epoch 6: val_accuracy did not improve from 0.71500\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.7543 - accuracy: 0.5575 - val_loss: 0.6275 - val_accuracy: 0.7125\n",
      "Epoch 7/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7193 - accuracy: 0.5708\n",
      "Epoch 7: val_accuracy improved from 0.71500 to 0.73750, saving model to ../Model/RNN_new/07-0.7375.h5\n",
      "50/50 [==============================] - 1s 10ms/step - loss: 0.7186 - accuracy: 0.5700 - val_loss: 0.6144 - val_accuracy: 0.7375\n",
      "Epoch 8/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.6855 - accuracy: 0.5795\n",
      "Epoch 8: val_accuracy improved from 0.73750 to 0.75000, saving model to ../Model/RNN_new/08-0.7500.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.6815 - accuracy: 0.5863 - val_loss: 0.5824 - val_accuracy: 0.7500\n",
      "Epoch 9/200\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.6853 - accuracy: 0.5951\n",
      "Epoch 9: val_accuracy improved from 0.75000 to 0.77250, saving model to ../Model/RNN_new/09-0.7725.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.6869 - accuracy: 0.5994 - val_loss: 0.5470 - val_accuracy: 0.7725\n",
      "Epoch 10/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6637 - accuracy: 0.6413\n",
      "Epoch 10: val_accuracy did not improve from 0.77250\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.6637 - accuracy: 0.6413 - val_loss: 0.5339 - val_accuracy: 0.7700\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6512 - accuracy: 0.6438\n",
      "Epoch 11: val_accuracy improved from 0.77250 to 0.78750, saving model to ../Model/RNN_new/11-0.7875.h5\n",
      "50/50 [==============================] - 0s 10ms/step - loss: 0.6512 - accuracy: 0.6438 - val_loss: 0.5232 - val_accuracy: 0.7875\n",
      "Epoch 12/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6416 - accuracy: 0.6600\n",
      "Epoch 12: val_accuracy improved from 0.78750 to 0.80000, saving model to ../Model/RNN_new/12-0.8000.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.6416 - accuracy: 0.6600 - val_loss: 0.5013 - val_accuracy: 0.8000\n",
      "Epoch 13/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6027 - accuracy: 0.6843\n",
      "Epoch 13: val_accuracy did not improve from 0.80000\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.6018 - accuracy: 0.6837 - val_loss: 0.4676 - val_accuracy: 0.8000\n",
      "Epoch 14/200\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.6032 - accuracy: 0.6854\n",
      "Epoch 14: val_accuracy improved from 0.80000 to 0.80750, saving model to ../Model/RNN_new/14-0.8075.h5\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.5991 - accuracy: 0.6913 - val_loss: 0.4619 - val_accuracy: 0.8075\n",
      "Epoch 15/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.6016 - accuracy: 0.6908\n",
      "Epoch 15: val_accuracy did not improve from 0.80750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.6019 - accuracy: 0.6875 - val_loss: 0.4520 - val_accuracy: 0.8050\n",
      "Epoch 16/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.5763 - accuracy: 0.7174\n",
      "Epoch 16: val_accuracy improved from 0.80750 to 0.83000, saving model to ../Model/RNN_new/16-0.8300.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.5789 - accuracy: 0.7200 - val_loss: 0.4361 - val_accuracy: 0.8300\n",
      "Epoch 17/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.5798 - accuracy: 0.7344\n",
      "Epoch 17: val_accuracy improved from 0.83000 to 0.83500, saving model to ../Model/RNN_new/17-0.8350.h5\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.5732 - accuracy: 0.7344 - val_loss: 0.4385 - val_accuracy: 0.8350\n",
      "Epoch 18/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.5605 - accuracy: 0.7307\n",
      "Epoch 18: val_accuracy improved from 0.83500 to 0.84250, saving model to ../Model/RNN_new/18-0.8425.h5\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.5729 - accuracy: 0.7269 - val_loss: 0.4050 - val_accuracy: 0.8425\n",
      "Epoch 19/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.5745 - accuracy: 0.7272\n",
      "Epoch 19: val_accuracy did not improve from 0.84250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.5725 - accuracy: 0.7275 - val_loss: 0.4203 - val_accuracy: 0.8400\n",
      "Epoch 20/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5197 - accuracy: 0.7588\n",
      "Epoch 20: val_accuracy did not improve from 0.84250\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.5197 - accuracy: 0.7588 - val_loss: 0.4127 - val_accuracy: 0.8400\n",
      "Epoch 21/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5100 - accuracy: 0.7768\n",
      "Epoch 21: val_accuracy did not improve from 0.84250\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.5133 - accuracy: 0.7769 - val_loss: 0.4191 - val_accuracy: 0.8275\n",
      "Epoch 22/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5039 - accuracy: 0.7691\n",
      "Epoch 22: val_accuracy did not improve from 0.84250\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.5041 - accuracy: 0.7700 - val_loss: 0.3895 - val_accuracy: 0.8425\n",
      "Epoch 23/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5063 - accuracy: 0.7819\n",
      "Epoch 23: val_accuracy improved from 0.84250 to 0.84750, saving model to ../Model/RNN_new/23-0.8475.h5\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.5029 - accuracy: 0.7837 - val_loss: 0.3916 - val_accuracy: 0.8475\n",
      "Epoch 24/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5100 - accuracy: 0.7927\n",
      "Epoch 24: val_accuracy improved from 0.84750 to 0.86250, saving model to ../Model/RNN_new/24-0.8625.h5\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.5067 - accuracy: 0.7956 - val_loss: 0.3847 - val_accuracy: 0.8625\n",
      "Epoch 25/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4820 - accuracy: 0.7927\n",
      "Epoch 25: val_accuracy did not improve from 0.86250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4813 - accuracy: 0.7912 - val_loss: 0.3774 - val_accuracy: 0.8625\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4656 - accuracy: 0.8094\n",
      "Epoch 26: val_accuracy did not improve from 0.86250\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.4656 - accuracy: 0.8094 - val_loss: 0.3899 - val_accuracy: 0.8550\n",
      "Epoch 27/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4720 - accuracy: 0.7991\n",
      "Epoch 27: val_accuracy did not improve from 0.86250\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.4769 - accuracy: 0.7969 - val_loss: 0.3759 - val_accuracy: 0.8575\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4775 - accuracy: 0.8044\n",
      "Epoch 28: val_accuracy improved from 0.86250 to 0.86750, saving model to ../Model/RNN_new/28-0.8675.h5\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4775 - accuracy: 0.8044 - val_loss: 0.3654 - val_accuracy: 0.8675\n",
      "Epoch 29/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4557 - accuracy: 0.8240\n",
      "Epoch 29: val_accuracy did not improve from 0.86750\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.4538 - accuracy: 0.8244 - val_loss: 0.3691 - val_accuracy: 0.8650\n",
      "Epoch 30/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4377 - accuracy: 0.8313\n",
      "Epoch 30: val_accuracy did not improve from 0.86750\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.4377 - accuracy: 0.8313 - val_loss: 0.3649 - val_accuracy: 0.8675\n",
      "Epoch 31/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4212 - accuracy: 0.8294\n",
      "Epoch 31: val_accuracy did not improve from 0.86750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4211 - accuracy: 0.8325 - val_loss: 0.3565 - val_accuracy: 0.8625\n",
      "Epoch 32/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.4165 - accuracy: 0.8376\n",
      "Epoch 32: val_accuracy did not improve from 0.86750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4292 - accuracy: 0.8350 - val_loss: 0.3542 - val_accuracy: 0.8600\n",
      "Epoch 33/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.4243 - accuracy: 0.8438\n",
      "Epoch 33: val_accuracy improved from 0.86750 to 0.88250, saving model to ../Model/RNN_new/33-0.8825.h5\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4337 - accuracy: 0.8413 - val_loss: 0.3559 - val_accuracy: 0.8825\n",
      "Epoch 34/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4272 - accuracy: 0.8392\n",
      "Epoch 34: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4254 - accuracy: 0.8400 - val_loss: 0.3757 - val_accuracy: 0.8625\n",
      "Epoch 35/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4005 - accuracy: 0.8469\n",
      "Epoch 35: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4036 - accuracy: 0.8462 - val_loss: 0.4168 - val_accuracy: 0.8525\n",
      "Epoch 36/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4565 - accuracy: 0.8275\n",
      "Epoch 36: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4565 - accuracy: 0.8275 - val_loss: 0.3688 - val_accuracy: 0.8700\n",
      "Epoch 37/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4077 - accuracy: 0.8509\n",
      "Epoch 37: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.4115 - accuracy: 0.8512 - val_loss: 0.3693 - val_accuracy: 0.8700\n",
      "Epoch 38/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4196 - accuracy: 0.8548\n",
      "Epoch 38: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4154 - accuracy: 0.8556 - val_loss: 0.3644 - val_accuracy: 0.8750\n",
      "Epoch 39/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4188 - accuracy: 0.8577\n",
      "Epoch 39: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4125 - accuracy: 0.8594 - val_loss: 0.3474 - val_accuracy: 0.8825\n",
      "Epoch 40/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3987 - accuracy: 0.8531\n",
      "Epoch 40: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4004 - accuracy: 0.8506 - val_loss: 0.3670 - val_accuracy: 0.8725\n",
      "Epoch 41/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4079 - accuracy: 0.8457\n",
      "Epoch 41: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4089 - accuracy: 0.8462 - val_loss: 0.3709 - val_accuracy: 0.8700\n",
      "Epoch 42/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.3864 - accuracy: 0.8594\n",
      "Epoch 42: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3779 - accuracy: 0.8612 - val_loss: 0.3475 - val_accuracy: 0.8800\n",
      "Epoch 43/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3636 - accuracy: 0.8617\n",
      "Epoch 43: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3697 - accuracy: 0.8587 - val_loss: 0.3780 - val_accuracy: 0.8675\n",
      "Epoch 44/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3800 - accuracy: 0.8597\n",
      "Epoch 44: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3778 - accuracy: 0.8569 - val_loss: 0.3951 - val_accuracy: 0.8600\n",
      "Epoch 45/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4177 - accuracy: 0.8457\n",
      "Epoch 45: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.4229 - accuracy: 0.8444 - val_loss: 0.3605 - val_accuracy: 0.8825\n",
      "Epoch 46/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.3607 - accuracy: 0.8709\n",
      "Epoch 46: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3701 - accuracy: 0.8669 - val_loss: 0.4016 - val_accuracy: 0.8425\n",
      "Epoch 47/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3513 - accuracy: 0.8730\n",
      "Epoch 47: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3547 - accuracy: 0.8737 - val_loss: 0.3648 - val_accuracy: 0.8675\n",
      "Epoch 48/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3394 - accuracy: 0.8843\n",
      "Epoch 48: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3452 - accuracy: 0.8800 - val_loss: 0.3692 - val_accuracy: 0.8700\n",
      "Epoch 49/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3750 - accuracy: 0.8517\n",
      "Epoch 49: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3721 - accuracy: 0.8519 - val_loss: 0.4019 - val_accuracy: 0.8625\n",
      "Epoch 50/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3861 - accuracy: 0.8584\n",
      "Epoch 50: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3831 - accuracy: 0.8581 - val_loss: 0.3757 - val_accuracy: 0.8700\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3089 - accuracy: 0.8944\n",
      "Epoch 51: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3089 - accuracy: 0.8944 - val_loss: 0.4014 - val_accuracy: 0.8675\n",
      "Epoch 52/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3161 - accuracy: 0.8816\n",
      "Epoch 52: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3191 - accuracy: 0.8813 - val_loss: 0.3918 - val_accuracy: 0.8700\n",
      "Epoch 53/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2978 - accuracy: 0.8913\n",
      "Epoch 53: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2963 - accuracy: 0.8906 - val_loss: 0.4334 - val_accuracy: 0.8550\n",
      "Epoch 54/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2989 - accuracy: 0.8997\n",
      "Epoch 54: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.3067 - accuracy: 0.8988 - val_loss: 0.4221 - val_accuracy: 0.8600\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.8956\n",
      "Epoch 55: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2878 - accuracy: 0.8956 - val_loss: 0.4015 - val_accuracy: 0.8625\n",
      "Epoch 56/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2955 - accuracy: 0.8931\n",
      "Epoch 56: val_accuracy did not improve from 0.88250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2955 - accuracy: 0.8931 - val_loss: 0.4259 - val_accuracy: 0.8725\n",
      "Epoch 57/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2754 - accuracy: 0.9082\n",
      "Epoch 57: val_accuracy improved from 0.88250 to 0.88750, saving model to ../Model/RNN_new/57-0.8875.h5\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.2835 - accuracy: 0.9069 - val_loss: 0.3794 - val_accuracy: 0.8875\n",
      "Epoch 58/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.3195 - accuracy: 0.8906\n",
      "Epoch 58: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.3193 - accuracy: 0.8913 - val_loss: 0.3912 - val_accuracy: 0.8725\n",
      "Epoch 59/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2798 - accuracy: 0.8870\n",
      "Epoch 59: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2866 - accuracy: 0.8856 - val_loss: 0.3761 - val_accuracy: 0.8850\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.8963\n",
      "Epoch 60: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2929 - accuracy: 0.8963 - val_loss: 0.4143 - val_accuracy: 0.8600\n",
      "Epoch 61/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2564 - accuracy: 0.9114\n",
      "Epoch 61: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2557 - accuracy: 0.9112 - val_loss: 0.4445 - val_accuracy: 0.8800\n",
      "Epoch 62/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2688 - accuracy: 0.9102\n",
      "Epoch 62: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2744 - accuracy: 0.9075 - val_loss: 0.3925 - val_accuracy: 0.8750\n",
      "Epoch 63/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.2879 - accuracy: 0.8893\n",
      "Epoch 63: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2986 - accuracy: 0.8919 - val_loss: 0.4707 - val_accuracy: 0.8325\n",
      "Epoch 64/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2846 - accuracy: 0.8969\n",
      "Epoch 64: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2785 - accuracy: 0.9006 - val_loss: 0.4251 - val_accuracy: 0.8650\n",
      "Epoch 65/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2735 - accuracy: 0.9023\n",
      "Epoch 65: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2725 - accuracy: 0.9025 - val_loss: 0.4174 - val_accuracy: 0.8775\n",
      "Epoch 66/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2650 - accuracy: 0.8936\n",
      "Epoch 66: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2635 - accuracy: 0.8919 - val_loss: 0.4182 - val_accuracy: 0.8775\n",
      "Epoch 67/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2450 - accuracy: 0.9096\n",
      "Epoch 67: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2428 - accuracy: 0.9125 - val_loss: 0.4316 - val_accuracy: 0.8775\n",
      "Epoch 68/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2177 - accuracy: 0.9222\n",
      "Epoch 68: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2262 - accuracy: 0.9200 - val_loss: 0.4355 - val_accuracy: 0.8725\n",
      "Epoch 69/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2441 - accuracy: 0.9134\n",
      "Epoch 69: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2406 - accuracy: 0.9137 - val_loss: 0.4735 - val_accuracy: 0.8675\n",
      "Epoch 70/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.2304 - accuracy: 0.9137\n",
      "Epoch 70: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2339 - accuracy: 0.9137 - val_loss: 0.5103 - val_accuracy: 0.8425\n",
      "Epoch 71/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2476 - accuracy: 0.9082\n",
      "Epoch 71: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2482 - accuracy: 0.9100 - val_loss: 0.4932 - val_accuracy: 0.8725\n",
      "Epoch 72/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2172 - accuracy: 0.9216\n",
      "Epoch 72: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2215 - accuracy: 0.9206 - val_loss: 0.5030 - val_accuracy: 0.8575\n",
      "Epoch 73/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.2324 - accuracy: 0.9029\n",
      "Epoch 73: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2352 - accuracy: 0.9031 - val_loss: 0.5176 - val_accuracy: 0.8525\n",
      "Epoch 74/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2241 - accuracy: 0.9114\n",
      "Epoch 74: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2220 - accuracy: 0.9125 - val_loss: 0.5292 - val_accuracy: 0.8750\n",
      "Epoch 75/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2392 - accuracy: 0.9031\n",
      "Epoch 75: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2391 - accuracy: 0.9038 - val_loss: 0.5184 - val_accuracy: 0.8500\n",
      "Epoch 76/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.2477 - accuracy: 0.9083\n",
      "Epoch 76: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2433 - accuracy: 0.9087 - val_loss: 0.5284 - val_accuracy: 0.8575\n",
      "Epoch 77/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2551 - accuracy: 0.9116\n",
      "Epoch 77: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2560 - accuracy: 0.9106 - val_loss: 0.4489 - val_accuracy: 0.8825\n",
      "Epoch 78/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3012 - accuracy: 0.8923\n",
      "Epoch 78: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2947 - accuracy: 0.8956 - val_loss: 0.5864 - val_accuracy: 0.8375\n",
      "Epoch 79/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.3018 - accuracy: 0.8825\n",
      "Epoch 79: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2999 - accuracy: 0.8806 - val_loss: 0.4610 - val_accuracy: 0.8650\n",
      "Epoch 80/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.2342 - accuracy: 0.9110\n",
      "Epoch 80: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2289 - accuracy: 0.9119 - val_loss: 0.4955 - val_accuracy: 0.8750\n",
      "Epoch 81/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2110 - accuracy: 0.9141\n",
      "Epoch 81: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2099 - accuracy: 0.9137 - val_loss: 0.5394 - val_accuracy: 0.8575\n",
      "Epoch 82/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2055 - accuracy: 0.9175\n",
      "Epoch 82: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2055 - accuracy: 0.9175 - val_loss: 0.6461 - val_accuracy: 0.8700\n",
      "Epoch 83/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2144 - accuracy: 0.9167\n",
      "Epoch 83: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2174 - accuracy: 0.9169 - val_loss: 0.5529 - val_accuracy: 0.8575\n",
      "Epoch 84/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1968 - accuracy: 0.9156\n",
      "Epoch 84: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1984 - accuracy: 0.9175 - val_loss: 0.5958 - val_accuracy: 0.8700\n",
      "Epoch 85/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2142 - accuracy: 0.9133\n",
      "Epoch 85: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2165 - accuracy: 0.9125 - val_loss: 0.6252 - val_accuracy: 0.8575\n",
      "Epoch 86/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2287 - accuracy: 0.9114\n",
      "Epoch 86: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2305 - accuracy: 0.9100 - val_loss: 0.6340 - val_accuracy: 0.8725\n",
      "Epoch 87/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2381 - accuracy: 0.9043\n",
      "Epoch 87: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2354 - accuracy: 0.9038 - val_loss: 0.5380 - val_accuracy: 0.8650\n",
      "Epoch 88/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2318 - accuracy: 0.9062\n",
      "Epoch 88: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2279 - accuracy: 0.9075 - val_loss: 0.5881 - val_accuracy: 0.8625\n",
      "Epoch 89/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2030 - accuracy: 0.9167\n",
      "Epoch 89: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1998 - accuracy: 0.9181 - val_loss: 0.5751 - val_accuracy: 0.8725\n",
      "Epoch 90/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1931 - accuracy: 0.9239\n",
      "Epoch 90: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1931 - accuracy: 0.9187 - val_loss: 0.5965 - val_accuracy: 0.8675\n",
      "Epoch 91/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1803 - accuracy: 0.9229\n",
      "Epoch 91: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1816 - accuracy: 0.9237 - val_loss: 0.6315 - val_accuracy: 0.8525\n",
      "Epoch 92/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1820 - accuracy: 0.9260\n",
      "Epoch 92: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1816 - accuracy: 0.9250 - val_loss: 0.6573 - val_accuracy: 0.8700\n",
      "Epoch 93/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1875 - accuracy: 0.9309\n",
      "Epoch 93: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1933 - accuracy: 0.9287 - val_loss: 0.7469 - val_accuracy: 0.8750\n",
      "Epoch 94/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1837 - accuracy: 0.9209\n",
      "Epoch 94: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1871 - accuracy: 0.9206 - val_loss: 0.7244 - val_accuracy: 0.8650\n",
      "Epoch 95/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1819 - accuracy: 0.9199\n",
      "Epoch 95: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1798 - accuracy: 0.9200 - val_loss: 0.7573 - val_accuracy: 0.8700\n",
      "Epoch 96/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1704 - accuracy: 0.9310\n",
      "Epoch 96: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1711 - accuracy: 0.9312 - val_loss: 0.8700 - val_accuracy: 0.8200\n",
      "Epoch 97/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1700 - accuracy: 0.9273\n",
      "Epoch 97: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1688 - accuracy: 0.9275 - val_loss: 0.8648 - val_accuracy: 0.8575\n",
      "Epoch 98/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1662 - accuracy: 0.9262\n",
      "Epoch 98: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1666 - accuracy: 0.9262 - val_loss: 1.0802 - val_accuracy: 0.7875\n",
      "Epoch 99/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1830 - accuracy: 0.9264\n",
      "Epoch 99: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1802 - accuracy: 0.9269 - val_loss: 0.9159 - val_accuracy: 0.8600\n",
      "Epoch 100/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1883 - accuracy: 0.9315\n",
      "Epoch 100: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1846 - accuracy: 0.9312 - val_loss: 0.7550 - val_accuracy: 0.8650\n",
      "Epoch 101/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1964 - accuracy: 0.9215\n",
      "Epoch 101: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1973 - accuracy: 0.9225 - val_loss: 0.7513 - val_accuracy: 0.8600\n",
      "Epoch 102/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2292 - accuracy: 0.9186\n",
      "Epoch 102: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2234 - accuracy: 0.9206 - val_loss: 0.9054 - val_accuracy: 0.8625\n",
      "Epoch 103/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2359 - accuracy: 0.9141\n",
      "Epoch 103: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2426 - accuracy: 0.9119 - val_loss: 0.6813 - val_accuracy: 0.8425\n",
      "Epoch 104/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2410 - accuracy: 0.8923\n",
      "Epoch 104: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2383 - accuracy: 0.8944 - val_loss: 0.5908 - val_accuracy: 0.8675\n",
      "Epoch 105/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2096 - accuracy: 0.9154\n",
      "Epoch 105: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.2089 - accuracy: 0.9144 - val_loss: 0.6480 - val_accuracy: 0.8325\n",
      "Epoch 106/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1769 - accuracy: 0.9222\n",
      "Epoch 106: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1775 - accuracy: 0.9206 - val_loss: 0.6022 - val_accuracy: 0.8650\n",
      "Epoch 107/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1703 - accuracy: 0.9253\n",
      "Epoch 107: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1733 - accuracy: 0.9231 - val_loss: 0.6652 - val_accuracy: 0.8775\n",
      "Epoch 108/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1831 - accuracy: 0.9279\n",
      "Epoch 108: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1842 - accuracy: 0.9275 - val_loss: 0.7192 - val_accuracy: 0.8350\n",
      "Epoch 109/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1757 - accuracy: 0.9219\n",
      "Epoch 109: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1750 - accuracy: 0.9219 - val_loss: 0.6775 - val_accuracy: 0.8750\n",
      "Epoch 110/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1646 - accuracy: 0.9247\n",
      "Epoch 110: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1648 - accuracy: 0.9250 - val_loss: 0.7772 - val_accuracy: 0.8775\n",
      "Epoch 111/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1497 - accuracy: 0.9375\n",
      "Epoch 111: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1481 - accuracy: 0.9369 - val_loss: 0.7917 - val_accuracy: 0.8725\n",
      "Epoch 112/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1547 - accuracy: 0.9279\n",
      "Epoch 112: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1554 - accuracy: 0.9262 - val_loss: 0.8407 - val_accuracy: 0.8600\n",
      "Epoch 113/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1565 - accuracy: 0.9255\n",
      "Epoch 113: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1594 - accuracy: 0.9269 - val_loss: 0.7754 - val_accuracy: 0.8775\n",
      "Epoch 114/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1625 - accuracy: 0.9290\n",
      "Epoch 114: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1703 - accuracy: 0.9244 - val_loss: 0.7814 - val_accuracy: 0.8700\n",
      "Epoch 115/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1604 - accuracy: 0.9203\n",
      "Epoch 115: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1593 - accuracy: 0.9200 - val_loss: 0.7883 - val_accuracy: 0.8750\n",
      "Epoch 116/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1647 - accuracy: 0.9297\n",
      "Epoch 116: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1631 - accuracy: 0.9312 - val_loss: 0.7993 - val_accuracy: 0.8750\n",
      "Epoch 117/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1492 - accuracy: 0.9381\n",
      "Epoch 117: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1499 - accuracy: 0.9369 - val_loss: 0.8143 - val_accuracy: 0.8800\n",
      "Epoch 118/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1524 - accuracy: 0.9287\n",
      "Epoch 118: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1524 - accuracy: 0.9287 - val_loss: 0.8828 - val_accuracy: 0.8525\n",
      "Epoch 119/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1227 - accuracy: 0.9315\n",
      "Epoch 119: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1267 - accuracy: 0.9325 - val_loss: 0.8944 - val_accuracy: 0.8600\n",
      "Epoch 120/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1592 - accuracy: 0.9235\n",
      "Epoch 120: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1574 - accuracy: 0.9244 - val_loss: 0.8882 - val_accuracy: 0.8800\n",
      "Epoch 121/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1363 - accuracy: 0.9362\n",
      "Epoch 121: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1354 - accuracy: 0.9369 - val_loss: 0.9068 - val_accuracy: 0.8800\n",
      "Epoch 122/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1322 - accuracy: 0.9289\n",
      "Epoch 122: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1330 - accuracy: 0.9269 - val_loss: 0.9497 - val_accuracy: 0.8800\n",
      "Epoch 123/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9297\n",
      "Epoch 123: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1308 - accuracy: 0.9312 - val_loss: 0.9675 - val_accuracy: 0.8750\n",
      "Epoch 124/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9349\n",
      "Epoch 124: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1310 - accuracy: 0.9369 - val_loss: 1.0211 - val_accuracy: 0.8775\n",
      "Epoch 125/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1386 - accuracy: 0.9262\n",
      "Epoch 125: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1370 - accuracy: 0.9294 - val_loss: 1.0502 - val_accuracy: 0.8775\n",
      "Epoch 126/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1255 - accuracy: 0.9428\n",
      "Epoch 126: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1291 - accuracy: 0.9425 - val_loss: 1.0689 - val_accuracy: 0.8725\n",
      "Epoch 127/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1267 - accuracy: 0.9258\n",
      "Epoch 127: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1257 - accuracy: 0.9269 - val_loss: 1.0842 - val_accuracy: 0.8725\n",
      "Epoch 128/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1343 - accuracy: 0.9229\n",
      "Epoch 128: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1333 - accuracy: 0.9244 - val_loss: 1.2208 - val_accuracy: 0.8425\n",
      "Epoch 129/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1234 - accuracy: 0.9303\n",
      "Epoch 129: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1227 - accuracy: 0.9306 - val_loss: 1.1531 - val_accuracy: 0.8750\n",
      "Epoch 130/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1330 - accuracy: 0.9302\n",
      "Epoch 130: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1307 - accuracy: 0.9319 - val_loss: 1.1399 - val_accuracy: 0.8750\n",
      "Epoch 131/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1774 - accuracy: 0.9242\n",
      "Epoch 131: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1799 - accuracy: 0.9225 - val_loss: 1.3178 - val_accuracy: 0.8275\n",
      "Epoch 132/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.2625 - accuracy: 0.8974\n",
      "Epoch 132: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2752 - accuracy: 0.8956 - val_loss: 0.8362 - val_accuracy: 0.8700\n",
      "Epoch 133/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2127 - accuracy: 0.8996\n",
      "Epoch 133: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.2126 - accuracy: 0.9000 - val_loss: 0.8038 - val_accuracy: 0.8450\n",
      "Epoch 134/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1667 - accuracy: 0.9295\n",
      "Epoch 134: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1654 - accuracy: 0.9300 - val_loss: 0.9019 - val_accuracy: 0.8725\n",
      "Epoch 135/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1329 - accuracy: 0.9295\n",
      "Epoch 135: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1353 - accuracy: 0.9262 - val_loss: 0.9846 - val_accuracy: 0.8400\n",
      "Epoch 136/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1371 - accuracy: 0.9307\n",
      "Epoch 136: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1375 - accuracy: 0.9300 - val_loss: 1.0716 - val_accuracy: 0.8300\n",
      "Epoch 137/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9290\n",
      "Epoch 137: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1252 - accuracy: 0.9287 - val_loss: 1.0420 - val_accuracy: 0.8725\n",
      "Epoch 138/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9289\n",
      "Epoch 138: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1288 - accuracy: 0.9287 - val_loss: 1.2328 - val_accuracy: 0.8300\n",
      "Epoch 139/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9473\n",
      "Epoch 139: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1144 - accuracy: 0.9488 - val_loss: 1.1553 - val_accuracy: 0.8600\n",
      "Epoch 140/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1334 - accuracy: 0.9287\n",
      "Epoch 140: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1318 - accuracy: 0.9294 - val_loss: 1.2384 - val_accuracy: 0.8500\n",
      "Epoch 141/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1370 - accuracy: 0.9182\n",
      "Epoch 141: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1348 - accuracy: 0.9212 - val_loss: 1.3379 - val_accuracy: 0.8300\n",
      "Epoch 142/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1187 - accuracy: 0.9421\n",
      "Epoch 142: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1182 - accuracy: 0.9425 - val_loss: 1.3780 - val_accuracy: 0.8225\n",
      "Epoch 143/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9388\n",
      "Epoch 143: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1196 - accuracy: 0.9400 - val_loss: 1.3829 - val_accuracy: 0.8350\n",
      "Epoch 144/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1236 - accuracy: 0.9254\n",
      "Epoch 144: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1239 - accuracy: 0.9250 - val_loss: 1.3916 - val_accuracy: 0.8350\n",
      "Epoch 145/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9349\n",
      "Epoch 145: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1244 - accuracy: 0.9350 - val_loss: 1.2949 - val_accuracy: 0.8575\n",
      "Epoch 146/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1904 - accuracy: 0.9225\n",
      "Epoch 146: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1883 - accuracy: 0.9212 - val_loss: 1.1487 - val_accuracy: 0.8125\n",
      "Epoch 147/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1553 - accuracy: 0.9206\n",
      "Epoch 147: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1570 - accuracy: 0.9187 - val_loss: 1.1449 - val_accuracy: 0.8300\n",
      "Epoch 148/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1714 - accuracy: 0.9232\n",
      "Epoch 148: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1696 - accuracy: 0.9237 - val_loss: 0.8483 - val_accuracy: 0.8675\n",
      "Epoch 149/200\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.1236 - accuracy: 0.9375\n",
      "Epoch 149: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1253 - accuracy: 0.9350 - val_loss: 0.8715 - val_accuracy: 0.8525\n",
      "Epoch 150/200\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.1282 - accuracy: 0.9264\n",
      "Epoch 150: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1254 - accuracy: 0.9281 - val_loss: 0.9026 - val_accuracy: 0.8525\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1152 - accuracy: 0.9381\n",
      "Epoch 151: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1152 - accuracy: 0.9381 - val_loss: 1.0457 - val_accuracy: 0.8575\n",
      "Epoch 152/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1101 - accuracy: 0.9395\n",
      "Epoch 152: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1119 - accuracy: 0.9388 - val_loss: 1.1452 - val_accuracy: 0.8350\n",
      "Epoch 153/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9232\n",
      "Epoch 153: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1199 - accuracy: 0.9206 - val_loss: 1.1567 - val_accuracy: 0.8525\n",
      "Epoch 154/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1162 - accuracy: 0.9316\n",
      "Epoch 154: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1154 - accuracy: 0.9331 - val_loss: 1.1938 - val_accuracy: 0.8600\n",
      "Epoch 155/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9302\n",
      "Epoch 155: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1146 - accuracy: 0.9312 - val_loss: 1.2328 - val_accuracy: 0.8600\n",
      "Epoch 156/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1080 - accuracy: 0.9355\n",
      "Epoch 156: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1070 - accuracy: 0.9350 - val_loss: 1.2749 - val_accuracy: 0.8625\n",
      "Epoch 157/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1103 - accuracy: 0.9362\n",
      "Epoch 157: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1097 - accuracy: 0.9356 - val_loss: 1.3219 - val_accuracy: 0.8575\n",
      "Epoch 158/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1037 - accuracy: 0.9355\n",
      "Epoch 158: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1035 - accuracy: 0.9350 - val_loss: 1.3507 - val_accuracy: 0.8600\n",
      "Epoch 159/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9402\n",
      "Epoch 159: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1070 - accuracy: 0.9381 - val_loss: 1.3842 - val_accuracy: 0.8650\n",
      "Epoch 160/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0997 - accuracy: 0.9448\n",
      "Epoch 160: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1012 - accuracy: 0.9425 - val_loss: 1.4102 - val_accuracy: 0.8650\n",
      "Epoch 161/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9282\n",
      "Epoch 161: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1204 - accuracy: 0.9275 - val_loss: 1.4376 - val_accuracy: 0.8675\n",
      "Epoch 162/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1030 - accuracy: 0.9388\n",
      "Epoch 162: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1031 - accuracy: 0.9375 - val_loss: 1.4594 - val_accuracy: 0.8675\n",
      "Epoch 163/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1062 - accuracy: 0.9423\n",
      "Epoch 163: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1064 - accuracy: 0.9413 - val_loss: 1.4888 - val_accuracy: 0.8675\n",
      "Epoch 164/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1171 - accuracy: 0.9219\n",
      "Epoch 164: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1175 - accuracy: 0.9212 - val_loss: 1.5164 - val_accuracy: 0.8675\n",
      "Epoch 165/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0960 - accuracy: 0.9434\n",
      "Epoch 165: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0959 - accuracy: 0.9438 - val_loss: 1.5469 - val_accuracy: 0.8675\n",
      "Epoch 166/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1024 - accuracy: 0.9447\n",
      "Epoch 166: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1028 - accuracy: 0.9438 - val_loss: 1.5725 - val_accuracy: 0.8675\n",
      "Epoch 167/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1074 - accuracy: 0.9330\n",
      "Epoch 167: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1064 - accuracy: 0.9344 - val_loss: 1.5928 - val_accuracy: 0.8700\n",
      "Epoch 168/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1101 - accuracy: 0.9324\n",
      "Epoch 168: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1099 - accuracy: 0.9319 - val_loss: 1.6119 - val_accuracy: 0.8625\n",
      "Epoch 169/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9375\n",
      "Epoch 169: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1066 - accuracy: 0.9375 - val_loss: 1.6376 - val_accuracy: 0.8650\n",
      "Epoch 170/200\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.1153 - accuracy: 0.9264\n",
      "Epoch 170: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1143 - accuracy: 0.9287 - val_loss: 1.6624 - val_accuracy: 0.8675\n",
      "Epoch 171/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1186 - accuracy: 0.9247\n",
      "Epoch 171: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1169 - accuracy: 0.9262 - val_loss: 1.6902 - val_accuracy: 0.8675\n",
      "Epoch 172/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0974 - accuracy: 0.9504\n",
      "Epoch 172: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0951 - accuracy: 0.9525 - val_loss: 1.7102 - val_accuracy: 0.8700\n",
      "Epoch 173/200\n",
      "45/50 [==========================>...] - ETA: 0s - loss: 0.1117 - accuracy: 0.9333\n",
      "Epoch 173: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 11ms/step - loss: 0.1107 - accuracy: 0.9325 - val_loss: 1.7326 - val_accuracy: 0.8700\n",
      "Epoch 174/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1040 - accuracy: 0.9395\n",
      "Epoch 174: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1033 - accuracy: 0.9406 - val_loss: 1.7520 - val_accuracy: 0.8725\n",
      "Epoch 175/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1072 - accuracy: 0.9395\n",
      "Epoch 175: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1064 - accuracy: 0.9400 - val_loss: 1.7747 - val_accuracy: 0.8725\n",
      "Epoch 176/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9343\n",
      "Epoch 176: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1056 - accuracy: 0.9337 - val_loss: 1.8009 - val_accuracy: 0.8725\n",
      "Epoch 177/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0921 - accuracy: 0.9447\n",
      "Epoch 177: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0902 - accuracy: 0.9456 - val_loss: 1.8215 - val_accuracy: 0.8725\n",
      "Epoch 178/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9464\n",
      "Epoch 178: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0916 - accuracy: 0.9469 - val_loss: 1.8468 - val_accuracy: 0.8750\n",
      "Epoch 179/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1061 - accuracy: 0.9355\n",
      "Epoch 179: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1072 - accuracy: 0.9337 - val_loss: 1.8114 - val_accuracy: 0.8725\n",
      "Epoch 180/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0942 - accuracy: 0.9422\n",
      "Epoch 180: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0954 - accuracy: 0.9419 - val_loss: 1.8424 - val_accuracy: 0.8750\n",
      "Epoch 181/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1035 - accuracy: 0.9414\n",
      "Epoch 181: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1026 - accuracy: 0.9413 - val_loss: 1.8704 - val_accuracy: 0.8725\n",
      "Epoch 182/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0928 - accuracy: 0.9505\n",
      "Epoch 182: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0927 - accuracy: 0.9506 - val_loss: 1.9011 - val_accuracy: 0.8725\n",
      "Epoch 183/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.0967 - accuracy: 0.9436\n",
      "Epoch 183: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0991 - accuracy: 0.9425 - val_loss: 1.9185 - val_accuracy: 0.8700\n",
      "Epoch 184/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1016 - accuracy: 0.9408\n",
      "Epoch 184: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1024 - accuracy: 0.9394 - val_loss: 1.9536 - val_accuracy: 0.8725\n",
      "Epoch 185/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1040 - accuracy: 0.9382\n",
      "Epoch 185: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1057 - accuracy: 0.9350 - val_loss: 1.9871 - val_accuracy: 0.8750\n",
      "Epoch 186/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1029 - accuracy: 0.9342\n",
      "Epoch 186: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1017 - accuracy: 0.9362 - val_loss: 2.0141 - val_accuracy: 0.8750\n",
      "Epoch 187/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1026 - accuracy: 0.9342\n",
      "Epoch 187: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1019 - accuracy: 0.9356 - val_loss: 2.0455 - val_accuracy: 0.8750\n",
      "Epoch 188/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0983 - accuracy: 0.9440\n",
      "Epoch 188: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0993 - accuracy: 0.9419 - val_loss: 2.0724 - val_accuracy: 0.8700\n",
      "Epoch 189/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1158 - accuracy: 0.9282\n",
      "Epoch 189: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1144 - accuracy: 0.9287 - val_loss: 2.1425 - val_accuracy: 0.8750\n",
      "Epoch 190/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.0960 - accuracy: 0.9455\n",
      "Epoch 190: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.0936 - accuracy: 0.9469 - val_loss: 2.1766 - val_accuracy: 0.8750\n",
      "Epoch 191/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1020 - accuracy: 0.9310\n",
      "Epoch 191: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1018 - accuracy: 0.9312 - val_loss: 2.2009 - val_accuracy: 0.8750\n",
      "Epoch 192/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0902 - accuracy: 0.9407\n",
      "Epoch 192: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0909 - accuracy: 0.9388 - val_loss: 2.2223 - val_accuracy: 0.8750\n",
      "Epoch 193/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0895 - accuracy: 0.9473\n",
      "Epoch 193: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0890 - accuracy: 0.9469 - val_loss: 2.2403 - val_accuracy: 0.8750\n",
      "Epoch 194/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9369\n",
      "Epoch 194: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0999 - accuracy: 0.9356 - val_loss: 2.2627 - val_accuracy: 0.8750\n",
      "Epoch 195/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.1020 - accuracy: 0.9388\n",
      "Epoch 195: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1023 - accuracy: 0.9388 - val_loss: 2.2843 - val_accuracy: 0.8750\n",
      "Epoch 196/200\n",
      "46/50 [==========================>...] - ETA: 0s - loss: 0.1004 - accuracy: 0.9334\n",
      "Epoch 196: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1014 - accuracy: 0.9337 - val_loss: 2.3054 - val_accuracy: 0.8750\n",
      "Epoch 197/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1077 - accuracy: 0.9342\n",
      "Epoch 197: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 13ms/step - loss: 0.1059 - accuracy: 0.9337 - val_loss: 2.3248 - val_accuracy: 0.8750\n",
      "Epoch 198/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.0954 - accuracy: 0.9368\n",
      "Epoch 198: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.0955 - accuracy: 0.9375 - val_loss: 2.3502 - val_accuracy: 0.8775\n",
      "Epoch 199/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1036 - accuracy: 0.9306\n",
      "Epoch 199: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1036 - accuracy: 0.9306 - val_loss: 2.3768 - val_accuracy: 0.8775\n",
      "Epoch 200/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.1058 - accuracy: 0.9289\n",
      "Epoch 200: val_accuracy did not improve from 0.88750\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.1053 - accuracy: 0.9287 - val_loss: 2.3986 - val_accuracy: 0.8775\n"
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    H=model.fit(X_train,yTrain,\n",
    "            validation_data=(X_test, yTest),\n",
    "            epochs=200,batch_size=32,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6f1c75ab50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 200\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"test_accuracy\")\n",
    "plt.title(\"Qubit State Classification RNN, 1\\u03BCs (0.5ns SR) [2000-4000]\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 18:15:44.645567: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-31 18:15:45.388500: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46113 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:19:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-31 18:15:48.008947: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8401\n",
      "2023-07-31 18:15:48.145313: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9551433]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01979821]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9377222]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02875638]]\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9092535]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9015065]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00874278]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94515115]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00821144]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9487119]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00577887]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.945148]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.01501568]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00543295]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9041124]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01684044]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00538544]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00353106]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9203185]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92701644]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.3886056]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.88374496]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.00525578]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92387676]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93636507]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01420847]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94103837]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00735337]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9078647]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00269991]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.5302607]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01910086]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9420297]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01699758]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.91663396]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.93480945]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93599385]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94772696]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94268596]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9414632]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00401737]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9200099]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03696187]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04556847]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.924665]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.10504525]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03270192]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00691542]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00510587]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9323504]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9484154]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.58345324]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9372301]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01827494]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.05928182]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.7664834]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9315524]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92643464]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9064246]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04056029]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.95207566]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01268651]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93373084]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.01593726]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9473106]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94135886]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9187399]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9383264]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00988406]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9523677]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9349965]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.14575931]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9498257]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9414688]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01201787]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02574251]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9238144]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03620772]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00657443]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9499935]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03697274]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.11845276]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9480918]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9330237]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9313316]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93586177]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9470691]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00978772]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04413212]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92316204]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9448547]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9449028]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.19202021]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.8744502]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9316895]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93872213]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9434727]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9507393]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93377274]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.0058234]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01947312]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00810952]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02582499]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93723005]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0082957]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.05564487]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00889109]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.7832963]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01324045]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01457317]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.849926]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93808967]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9441143]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92704225]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.10741933]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02486451]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.20928216]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00236691]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.18104693]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.83565795]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.8931313]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92726874]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92975825]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02524634]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01551688]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9321917]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.22869286]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.06301547]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9163501]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00661868]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9500302]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94269294]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01105234]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.8849649]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00295588]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9457493]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9491407]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.00418998]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.01515948]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.09289518]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94894785]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.94785374]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9143753]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00999927]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9018968]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00813083]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.16959843]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92383045]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00928188]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.06153456]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94901645]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0253765]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00337823]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93506914]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9430908]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.5679869]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.27937868]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.91090316]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00361038]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94159484]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01002389]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00294594]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03825824]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9445969]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9368178]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94800776]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94142216]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9290685]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0275322]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9368538]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9515799]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01374173]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.03782554]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.83922166]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01490921]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9385654]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.21921882]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9374095]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00454451]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9362784]]\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.862161]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04644458]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9433477]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9488383]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00802273]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01895078]]\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.88530564]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92809695]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9479582]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.08162549]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00768897]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01555958]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9221474]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.1311996]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.16717026]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00831802]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9330906]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00353179]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01392596]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9442983]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.934453]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01232181]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01909995]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.07741917]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0161783]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00299983]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92642707]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94671375]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9476519]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.11734477]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9363224]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00850593]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.17943963]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.26547316]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00524697]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92885333]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93833596]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.8060957]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9467976]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94323367]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.12789036]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02789922]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.84973234]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9560153]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.00274531]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.943806]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01556207]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00541185]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9379551]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.15096131]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9530118]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02956424]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.6117769]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9395013]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.87089825]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0054964]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94292045]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02746921]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9458668]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00298862]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.08787026]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94586724]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00441764]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00882676]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9238982]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01098907]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00420173]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94080204]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00528722]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9345086]]\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.8385086]]\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01391781]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.01565072]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00169847]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9319513]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.77129024]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00801848]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.93229896]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9479665]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.85261655]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.008907]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93881786]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.05826044]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93950635]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9455251]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93608814]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01066933]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00922475]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00514454]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9360916]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9272515]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.0170306]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.18819253]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93190455]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9307682]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9176292]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.47075164]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00514559]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94942975]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01204036]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.03294087]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94311184]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00686576]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94178677]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9496813]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.12657161]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9452911]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.857726]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.05814073]]\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01845968]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94162524]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93021303]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.920631]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9381521]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.06047581]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.02051133]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.92722905]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.81132597]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9294191]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00470458]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00895042]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00673467]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.16233677]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9328905]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01223011]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9502404]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.89612716]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9392464]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01216998]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.93628544]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94228697]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9491641]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.3791822]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.82511634]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9470423]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00512169]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9306634]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.03470166]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94364685]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.7984941]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.8897606]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9460375]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00589393]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.05971432]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01944673]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0169674]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9386408]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0132623]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9515291]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.21178488]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.11024704]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00737398]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9371204]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00659124]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01652048]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9276343]]\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0276057]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.11851442]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9410695]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9517617]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94556797]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0086353]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9313673]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00646514]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04336922]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94001096]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00232678]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9435203]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.95107156]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.09769841]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03865581]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00607872]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.01566923]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93834853]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00964393]]\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.01344057]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9335045]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9328786]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9458728]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9369856]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.37192044]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04839014]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.6413437]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9119066]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00676129]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03756582]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9528003]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.04401124]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9256368]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.17200424]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.8428789]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9496695]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.8825947]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.00798652]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.34097004]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9395391]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.05770172]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94976515]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9205149]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.92467284]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.95186895]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0244977]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0132603]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.7282117]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9527267]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.06766458]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9286218]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.94525117]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.00899034]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9457491]]\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.93253547]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.90821373]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.9457203]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.35065582]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.0047595]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.28500924]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.9090936]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  1.0 \n",
      "Prediction:-  [[0.947028]]\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.91920763]]\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "Ground Truth:-  0.0 \n",
      "Prediction:-  [[0.03433598]]\n"
     ]
    }
   ],
   "source": [
    "model=load_model(\"/home/neel/Berkely_Quantum/Model/RNN_new/57-0.8875.h5\", compile=False)\n",
    "p,g=[],[]\n",
    "for j,i in enumerate(X_test):\n",
    "    i=np.expand_dims(i,axis=0)\n",
    "    pred=model.predict(i)\n",
    "    print(\"Ground Truth:- \",yTest[j],\"\\nPrediction:- \",pred,)\n",
    "    p.append(float(pred))\n",
    "    g.append(yTest[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0\n",
    "for i in range(len(p)):\n",
    "    if (p[i]<0.5 and g[i]==0) or (p[i]>=0.5 and g[i]==1):\n",
    "        t+=1\n",
    "print((t*100)/len(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_score(p,g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantization and HLS4ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_2, layer type: InputLayer, input shapes: [[None, 10, 10]], output shape: [None, 10, 10]\n",
      "Layer name: lstm, layer type: LSTM, input shapes: [[None, 10, 10]], output shape: [None, 10, 64]\n",
      "Layer name: lstm_1, layer type: LSTM, input shapes: [[None, 10, 64]], output shape: [None, 64]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 16]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 4]\n",
      "Layer name: dense_9, layer type: Dense, input shapes: [[None, 4]], output shape: [None, 1]\n",
      "-----------------------------------\n",
      "Configuration\n",
      "-----------------------------------\n",
      "Interpreting Sequential\n",
      "Topology:\n",
      "Layer name: input_2, layer type: InputLayer, input shapes: [[None, 10, 10]], output shape: [None, 10, 10]\n",
      "Layer name: lstm, layer type: LSTM, input shapes: [[None, 10, 10]], output shape: [None, 10, 64]\n",
      "Layer name: lstm_1, layer type: LSTM, input shapes: [[None, 10, 64]], output shape: [None, 64]\n",
      "Layer name: dense_5, layer type: Dense, input shapes: [[None, 64]], output shape: [None, 64]\n",
      "Layer name: dense_6, layer type: Dense, input shapes: [[None, 64]], output shape: [None, 32]\n",
      "Layer name: dense_7, layer type: Dense, input shapes: [[None, 32]], output shape: [None, 16]\n",
      "Layer name: dense_8, layer type: Dense, input shapes: [[None, 16]], output shape: [None, 4]\n",
      "Layer name: dense_9, layer type: Dense, input shapes: [[None, 4]], output shape: [None, 1]\n",
      "Creating HLS model\n"
     ]
    }
   ],
   "source": [
    "from hls4ml.model import profiling\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='Model')\n",
    "config['Model']['Precision']='ap_fixed<8,2>'\n",
    "print(\"-----------------------------------\")\n",
    "print(\"Configuration\")\n",
    "print(\"-----------------------------------\")\n",
    "hls_model_std = hls4ml.converters.convert_from_keras_model(\n",
    "    model, hls_config=config, output_dir='model_2/hls4ml_prj', part='xczu49dr-2ffvf1760'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "hls4ml.utils.plot_model(hls_model_std, show_shapes=True, show_precision=True, to_file=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing HLS project\n",
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hls_model_std.compile()\n",
    "y_hls_std=hls_model_std.predict(np.ascontiguousarray(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 0s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "y_keras = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Vivado HLS installation not found. Make sure \"vivado_hls\" is on PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_44076/1509779926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhls_model_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hls4ml/model/graph.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 854\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/hls4ml/backends/vivado/vivado_backend.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, model, reset, csim, synth, cosim, validation, export, vsynth, fifo_opt)\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'command -v vivado_hls > /dev/null'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Vivado HLS installation not found. Make sure \"vivado_hls\" is on PATH.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mcurr_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Vivado HLS installation not found. Make sure \"vivado_hls\" is on PATH."
     ]
    }
   ],
   "source": [
    "hls_model_std.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
