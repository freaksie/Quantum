{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras import layers,Input\n",
    "from keras.models import  Sequential\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import L1,L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 40, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "xTrain=np.load(\"../Data/julyData/trainData.npy\")\n",
    "yTrain=np.load(\"../Data/julyData/trainTarget.npy\")\n",
    "xTest=np.load('../Data/julyData/testData.npy')\n",
    "yTest=np.load(\"../Data/julyData/testTarget.npy\")\n",
    "\n",
    "X_train=np.reshape(xTrain,(xTrain.shape[0],40,50,1))\n",
    "X_test=np.reshape(xTest,(xTest.shape[0],40,50,1))\n",
    "\n",
    "\n",
    "# yTrain=np.reshape(yTrain,(yTrain.shape[0]))\n",
    "# yTest=np.reshape(yTest,(yTest.shape[0]))\n",
    "# xTrain=np.reshape(xTrain,(xTrain.shape[0],20,25,1))\n",
    "# xTest=np.reshape(xTest,(xTest.shape[0],20,25,1))\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (1600, 200, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(xTrain)\n",
    "X_test = sc.transform(xTest)\n",
    "pca = PCA(n_components = 200)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)\n",
    "del xTest,xTrain\n",
    "X_train=np.reshape(X_train,(X_train.shape[0],200,1))\n",
    "X_test=np.reshape(X_test,(X_test.shape[0],200,1))\n",
    "print(\"X shape: {}\".format(X_train.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arch():\n",
    "    model=Sequential()\n",
    "    model.add(layers.Conv1D(16,kernel_size=3, padding='valid', strides=1, input_shape=(200,1),activation='relu',kernel_regularizer=L1L2(0.0001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(3,2))\n",
    "    model.add(layers.Conv1D(16,kernel_size=3, padding='valid', strides=1,activation='relu',kernel_regularizer=L1L2(0.0001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(3,2))\n",
    "    model.add(layers.Conv1D(16,kernel_size=3, padding='valid', strides=1,activation='relu',kernel_regularizer=L1L2(0.0001)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool1D(3,2))\n",
    "    model.add(layers.CuDNNLSTM(units=128,return_sequences=False))\n",
    "    model.add(layers.Dense(128,activation='relu'))\n",
    "    model.add(layers.Dropout(0.65))\n",
    "    model.add(layers.Dense(64,activation='relu'))\n",
    "    model.add(layers.Dropout(0.65))\n",
    "    model.add(layers.Dense(32,activation='relu'))\n",
    "    model.add(layers.Dropout(0.65))\n",
    "    model.add(layers.Dense(16,activation='relu'))\n",
    "    model.add(layers.Dropout(0.65))\n",
    "    model.add(layers.Dense(4,activation='relu'))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def arch2():\n",
    "    model=Sequential()\n",
    "    model.add(layers.Conv2D(16, (3,3), padding='valid', strides=2, input_shape=(40,50,1), activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPool2D((3,3), padding='valid', strides=2))\n",
    "    model.add(layers.Conv2D(16, (3,3), padding='valid', strides=2, activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Reshape((20,16)))\n",
    "    model.add(layers.CuDNNLSTM(units=16,return_sequences=False,kernel_regularizer=L1L2(0.0001)))\n",
    "    # model.add(layers.Dense(256, activation='relu'))\n",
    "    # model.add(layers.Dropout(0.2))\n",
    "    # model.add(layers.Dense(128, activation='relu'))\n",
    "    # model.add(layers.Dropout(0.2))\n",
    "    # model.add(layers.Dense(64, activation='relu'))\n",
    "    # model.add(layers.Dropout(0.2))\n",
    "    # model.add(layers.Dense(32, activation='relu'))\n",
    "    # model.add(layers.Dropout(0.2))\n",
    "    model.add(layers.Dense(16, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(8,activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "# arch2().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d (Conv1D)             (None, 198, 16)           64        \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 198, 16)          64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 98, 16)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 96, 16)            784       \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 96, 16)           64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 47, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 45, 16)            784       \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 45, 16)           64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 22, 16)           0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " cu_dnnlstm_4 (CuDNNLSTM)    (None, 128)               74752     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 16)                0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 4)                 68        \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 104,025\n",
      "Trainable params: 103,929\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model=arch()\n",
    "model.summary()\n",
    "opt=Adam( learning_rate=0.001)\n",
    "model.compile(loss=binary_crossentropy, optimizer=opt, metrics=['accuracy'])\n",
    "model_path=\"../Model/RNN_new/{epoch:02d}-{val_accuracy:.4f}.h5\"\n",
    "checkpoint = ModelCheckpoint(model_path, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7240 - accuracy: 0.5075\n",
      "Epoch 1: val_accuracy improved from -inf to 0.50250, saving model to ../Model/RNN_new/01-0.5025.h5\n",
      "50/50 [==============================] - 3s 28ms/step - loss: 0.7240 - accuracy: 0.5075 - val_loss: 0.7129 - val_accuracy: 0.5025\n",
      "Epoch 2/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7201 - accuracy: 0.4914\n",
      "Epoch 2: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7201 - accuracy: 0.4919 - val_loss: 0.7129 - val_accuracy: 0.5025\n",
      "Epoch 3/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7146 - accuracy: 0.5319\n",
      "Epoch 3: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7148 - accuracy: 0.5319 - val_loss: 0.7128 - val_accuracy: 0.5025\n",
      "Epoch 4/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7156 - accuracy: 0.5094\n",
      "Epoch 4: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7156 - accuracy: 0.5094 - val_loss: 0.7127 - val_accuracy: 0.5025\n",
      "Epoch 5/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7126 - accuracy: 0.5113\n",
      "Epoch 5: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7126 - accuracy: 0.5113 - val_loss: 0.7126 - val_accuracy: 0.5025\n",
      "Epoch 6/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7159 - accuracy: 0.5131\n",
      "Epoch 6: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7159 - accuracy: 0.5131 - val_loss: 0.7125 - val_accuracy: 0.5025\n",
      "Epoch 7/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.7139 - accuracy: 0.5124\n",
      "Epoch 7: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7136 - accuracy: 0.5131 - val_loss: 0.7123 - val_accuracy: 0.5025\n",
      "Epoch 8/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7141 - accuracy: 0.5113\n",
      "Epoch 8: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7141 - accuracy: 0.5113 - val_loss: 0.7121 - val_accuracy: 0.5025\n",
      "Epoch 9/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7131 - accuracy: 0.5033\n",
      "Epoch 9: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7132 - accuracy: 0.4988 - val_loss: 0.7120 - val_accuracy: 0.5025\n",
      "Epoch 10/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7120 - accuracy: 0.5134\n",
      "Epoch 10: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7120 - accuracy: 0.5138 - val_loss: 0.7117 - val_accuracy: 0.5025\n",
      "Epoch 11/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7111 - accuracy: 0.5088\n",
      "Epoch 11: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7111 - accuracy: 0.5088 - val_loss: 0.7115 - val_accuracy: 0.5025\n",
      "Epoch 12/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7144 - accuracy: 0.4898\n",
      "Epoch 12: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7146 - accuracy: 0.4931 - val_loss: 0.7113 - val_accuracy: 0.5025\n",
      "Epoch 13/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.4988\n",
      "Epoch 13: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7113 - accuracy: 0.4988 - val_loss: 0.7111 - val_accuracy: 0.5025\n",
      "Epoch 14/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7105 - accuracy: 0.5231\n",
      "Epoch 14: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7105 - accuracy: 0.5231 - val_loss: 0.7108 - val_accuracy: 0.5025\n",
      "Epoch 15/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7097 - accuracy: 0.5100\n",
      "Epoch 15: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.7096 - accuracy: 0.5094 - val_loss: 0.7106 - val_accuracy: 0.5025\n",
      "Epoch 16/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7118 - accuracy: 0.4898\n",
      "Epoch 16: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 12ms/step - loss: 0.7120 - accuracy: 0.4869 - val_loss: 0.7104 - val_accuracy: 0.5025\n",
      "Epoch 17/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7116 - accuracy: 0.4888\n",
      "Epoch 17: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7116 - accuracy: 0.4888 - val_loss: 0.7102 - val_accuracy: 0.5025\n",
      "Epoch 18/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7112 - accuracy: 0.5125\n",
      "Epoch 18: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7112 - accuracy: 0.5125 - val_loss: 0.7099 - val_accuracy: 0.5025\n",
      "Epoch 19/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7079 - accuracy: 0.5119\n",
      "Epoch 19: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7079 - accuracy: 0.5119 - val_loss: 0.7097 - val_accuracy: 0.5025\n",
      "Epoch 20/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7100 - accuracy: 0.4920\n",
      "Epoch 20: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7101 - accuracy: 0.4938 - val_loss: 0.7094 - val_accuracy: 0.5025\n",
      "Epoch 21/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7094 - accuracy: 0.5100\n",
      "Epoch 21: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7086 - accuracy: 0.5156 - val_loss: 0.7092 - val_accuracy: 0.5025\n",
      "Epoch 22/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7113 - accuracy: 0.4963\n",
      "Epoch 22: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7113 - accuracy: 0.4963 - val_loss: 0.7090 - val_accuracy: 0.5025\n",
      "Epoch 23/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7098 - accuracy: 0.5050\n",
      "Epoch 23: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7098 - accuracy: 0.5050 - val_loss: 0.7087 - val_accuracy: 0.5025\n",
      "Epoch 24/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7082 - accuracy: 0.5231\n",
      "Epoch 24: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7082 - accuracy: 0.5231 - val_loss: 0.7085 - val_accuracy: 0.5025\n",
      "Epoch 25/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7085 - accuracy: 0.5180\n",
      "Epoch 25: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7085 - accuracy: 0.5131 - val_loss: 0.7082 - val_accuracy: 0.5025\n",
      "Epoch 26/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.5213\n",
      "Epoch 26: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7076 - accuracy: 0.5213 - val_loss: 0.7080 - val_accuracy: 0.5025\n",
      "Epoch 27/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7073 - accuracy: 0.5044\n",
      "Epoch 27: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7073 - accuracy: 0.5044 - val_loss: 0.7077 - val_accuracy: 0.5025\n",
      "Epoch 28/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7082 - accuracy: 0.5075\n",
      "Epoch 28: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7082 - accuracy: 0.5075 - val_loss: 0.7075 - val_accuracy: 0.5025\n",
      "Epoch 29/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7076 - accuracy: 0.5156\n",
      "Epoch 29: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7076 - accuracy: 0.5156 - val_loss: 0.7072 - val_accuracy: 0.5025\n",
      "Epoch 30/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7080 - accuracy: 0.4994\n",
      "Epoch 30: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7080 - accuracy: 0.4988 - val_loss: 0.7070 - val_accuracy: 0.5025\n",
      "Epoch 31/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7063 - accuracy: 0.5175\n",
      "Epoch 31: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7063 - accuracy: 0.5175 - val_loss: 0.7067 - val_accuracy: 0.5025\n",
      "Epoch 32/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.7063 - accuracy: 0.5000\n",
      "Epoch 32: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7064 - accuracy: 0.5019 - val_loss: 0.7065 - val_accuracy: 0.5025\n",
      "Epoch 33/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7065 - accuracy: 0.4980\n",
      "Epoch 33: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7062 - accuracy: 0.5025 - val_loss: 0.7064 - val_accuracy: 0.5025\n",
      "Epoch 34/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7058 - accuracy: 0.5094\n",
      "Epoch 34: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7058 - accuracy: 0.5094 - val_loss: 0.7062 - val_accuracy: 0.5025\n",
      "Epoch 35/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.5050\n",
      "Epoch 35: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7057 - accuracy: 0.5050 - val_loss: 0.7059 - val_accuracy: 0.5025\n",
      "Epoch 36/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.7038 - accuracy: 0.5247\n",
      "Epoch 36: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7035 - accuracy: 0.5238 - val_loss: 0.7058 - val_accuracy: 0.5025\n",
      "Epoch 37/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7048 - accuracy: 0.5213\n",
      "Epoch 37: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7048 - accuracy: 0.5213 - val_loss: 0.7056 - val_accuracy: 0.5025\n",
      "Epoch 38/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7030 - accuracy: 0.5312\n",
      "Epoch 38: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7027 - accuracy: 0.5312 - val_loss: 0.7055 - val_accuracy: 0.5025\n",
      "Epoch 39/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7051 - accuracy: 0.5193\n",
      "Epoch 39: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7054 - accuracy: 0.5181 - val_loss: 0.7053 - val_accuracy: 0.5025\n",
      "Epoch 40/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7064 - accuracy: 0.5088\n",
      "Epoch 40: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7064 - accuracy: 0.5088 - val_loss: 0.7053 - val_accuracy: 0.5025\n",
      "Epoch 41/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7041 - accuracy: 0.5412\n",
      "Epoch 41: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7037 - accuracy: 0.5419 - val_loss: 0.7051 - val_accuracy: 0.5025\n",
      "Epoch 42/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.7036 - accuracy: 0.5124\n",
      "Epoch 42: val_accuracy did not improve from 0.50250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.7033 - accuracy: 0.5175 - val_loss: 0.7049 - val_accuracy: 0.5025\n",
      "Epoch 43/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.7003 - accuracy: 0.5273\n",
      "Epoch 43: val_accuracy improved from 0.50250 to 0.52500, saving model to ../Model/RNN_new/43-0.5250.h5\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.7006 - accuracy: 0.5238 - val_loss: 0.7042 - val_accuracy: 0.5250\n",
      "Epoch 44/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.6984 - accuracy: 0.5399\n",
      "Epoch 44: val_accuracy improved from 0.52500 to 0.55250, saving model to ../Model/RNN_new/44-0.5525.h5\n",
      "50/50 [==============================] - 1s 18ms/step - loss: 0.6982 - accuracy: 0.5425 - val_loss: 0.7038 - val_accuracy: 0.5525\n",
      "Epoch 45/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7008 - accuracy: 0.5494\n",
      "Epoch 45: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7008 - accuracy: 0.5494 - val_loss: 0.7041 - val_accuracy: 0.5275\n",
      "Epoch 46/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.7027 - accuracy: 0.5288\n",
      "Epoch 46: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.7027 - accuracy: 0.5288 - val_loss: 0.7042 - val_accuracy: 0.5250\n",
      "Epoch 47/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.7023 - accuracy: 0.5254\n",
      "Epoch 47: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.7020 - accuracy: 0.5256 - val_loss: 0.7035 - val_accuracy: 0.5400\n",
      "Epoch 48/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6989 - accuracy: 0.5523\n",
      "Epoch 48: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6990 - accuracy: 0.5519 - val_loss: 0.7038 - val_accuracy: 0.5325\n",
      "Epoch 49/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.6944 - accuracy: 0.5505\n",
      "Epoch 49: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6952 - accuracy: 0.5519 - val_loss: 0.7053 - val_accuracy: 0.4925\n",
      "Epoch 50/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.6924 - accuracy: 0.5791\n",
      "Epoch 50: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6934 - accuracy: 0.5706 - val_loss: 0.7061 - val_accuracy: 0.5025\n",
      "Epoch 51/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6868 - accuracy: 0.5675\n",
      "Epoch 51: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6868 - accuracy: 0.5675 - val_loss: 0.7063 - val_accuracy: 0.5000\n",
      "Epoch 52/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6984 - accuracy: 0.5487\n",
      "Epoch 52: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6984 - accuracy: 0.5487 - val_loss: 0.7068 - val_accuracy: 0.5025\n",
      "Epoch 53/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6918 - accuracy: 0.5638\n",
      "Epoch 53: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6918 - accuracy: 0.5638 - val_loss: 0.7068 - val_accuracy: 0.4875\n",
      "Epoch 54/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6886 - accuracy: 0.5806\n",
      "Epoch 54: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6886 - accuracy: 0.5806 - val_loss: 0.7062 - val_accuracy: 0.4900\n",
      "Epoch 55/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6815 - accuracy: 0.5763\n",
      "Epoch 55: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6815 - accuracy: 0.5763 - val_loss: 0.7048 - val_accuracy: 0.5125\n",
      "Epoch 56/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6788 - accuracy: 0.6027\n",
      "Epoch 56: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6803 - accuracy: 0.6044 - val_loss: 0.7043 - val_accuracy: 0.5025\n",
      "Epoch 57/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6700 - accuracy: 0.6276\n",
      "Epoch 57: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6700 - accuracy: 0.6275 - val_loss: 0.7061 - val_accuracy: 0.5050\n",
      "Epoch 58/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6668 - accuracy: 0.6150\n",
      "Epoch 58: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6668 - accuracy: 0.6150 - val_loss: 0.7072 - val_accuracy: 0.5025\n",
      "Epoch 59/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6530 - accuracy: 0.6306\n",
      "Epoch 59: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6530 - accuracy: 0.6306 - val_loss: 0.7110 - val_accuracy: 0.5150\n",
      "Epoch 60/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6731 - accuracy: 0.6119\n",
      "Epoch 60: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6731 - accuracy: 0.6119 - val_loss: 0.7083 - val_accuracy: 0.5050\n",
      "Epoch 61/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6577 - accuracy: 0.6175\n",
      "Epoch 61: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6577 - accuracy: 0.6175 - val_loss: 0.7102 - val_accuracy: 0.4950\n",
      "Epoch 62/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6438 - accuracy: 0.6206\n",
      "Epoch 62: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6438 - accuracy: 0.6206 - val_loss: 0.7116 - val_accuracy: 0.4975\n",
      "Epoch 63/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6528 - accuracy: 0.6375\n",
      "Epoch 63: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6528 - accuracy: 0.6375 - val_loss: 0.7052 - val_accuracy: 0.5450\n",
      "Epoch 64/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6537 - accuracy: 0.6582\n",
      "Epoch 64: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6541 - accuracy: 0.6569 - val_loss: 0.7026 - val_accuracy: 0.5350\n",
      "Epoch 65/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6340 - accuracy: 0.6712\n",
      "Epoch 65: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6340 - accuracy: 0.6712 - val_loss: 0.7100 - val_accuracy: 0.5025\n",
      "Epoch 66/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6334 - accuracy: 0.6569\n",
      "Epoch 66: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6338 - accuracy: 0.6562 - val_loss: 0.7249 - val_accuracy: 0.5125\n",
      "Epoch 67/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.6114 - accuracy: 0.6769\n",
      "Epoch 67: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6137 - accuracy: 0.6762 - val_loss: 0.7169 - val_accuracy: 0.4975\n",
      "Epoch 68/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.6116 - accuracy: 0.6760\n",
      "Epoch 68: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6136 - accuracy: 0.6725 - val_loss: 0.7174 - val_accuracy: 0.5025\n",
      "Epoch 69/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.6612\n",
      "Epoch 69: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6160 - accuracy: 0.6612 - val_loss: 0.7390 - val_accuracy: 0.5400\n",
      "Epoch 70/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.6212 - accuracy: 0.6656\n",
      "Epoch 70: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6197 - accuracy: 0.6650 - val_loss: 0.8197 - val_accuracy: 0.5300\n",
      "Epoch 71/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.6800\n",
      "Epoch 71: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.6168 - accuracy: 0.6800 - val_loss: 0.7679 - val_accuracy: 0.5400\n",
      "Epoch 72/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5795 - accuracy: 0.7075\n",
      "Epoch 72: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.5795 - accuracy: 0.7075 - val_loss: 0.7802 - val_accuracy: 0.5425\n",
      "Epoch 73/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.5866 - accuracy: 0.6901\n",
      "Epoch 73: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.5867 - accuracy: 0.6938 - val_loss: 0.8239 - val_accuracy: 0.5125\n",
      "Epoch 74/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5520 - accuracy: 0.7245\n",
      "Epoch 74: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.5510 - accuracy: 0.7250 - val_loss: 0.7702 - val_accuracy: 0.5325\n",
      "Epoch 75/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5330 - accuracy: 0.7455\n",
      "Epoch 75: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5326 - accuracy: 0.7450 - val_loss: 0.9038 - val_accuracy: 0.5250\n",
      "Epoch 76/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5459 - accuracy: 0.7287\n",
      "Epoch 76: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5459 - accuracy: 0.7287 - val_loss: 0.7952 - val_accuracy: 0.5400\n",
      "Epoch 77/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5393 - accuracy: 0.7337\n",
      "Epoch 77: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.5393 - accuracy: 0.7337 - val_loss: 0.9885 - val_accuracy: 0.5000\n",
      "Epoch 78/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5437 - accuracy: 0.7294\n",
      "Epoch 78: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.5437 - accuracy: 0.7294 - val_loss: 0.7934 - val_accuracy: 0.5175\n",
      "Epoch 79/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5838 - accuracy: 0.7015\n",
      "Epoch 79: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5845 - accuracy: 0.7019 - val_loss: 0.7294 - val_accuracy: 0.5100\n",
      "Epoch 80/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.5825 - accuracy: 0.6981\n",
      "Epoch 80: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5850 - accuracy: 0.6944 - val_loss: 0.7442 - val_accuracy: 0.5000\n",
      "Epoch 81/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.5707 - accuracy: 0.7013\n",
      "Epoch 81: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5707 - accuracy: 0.7013 - val_loss: 1.0591 - val_accuracy: 0.5000\n",
      "Epoch 82/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.5146 - accuracy: 0.7461\n",
      "Epoch 82: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.5147 - accuracy: 0.7462 - val_loss: 1.4536 - val_accuracy: 0.5250\n",
      "Epoch 83/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4934 - accuracy: 0.7660\n",
      "Epoch 83: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4922 - accuracy: 0.7688 - val_loss: 1.7868 - val_accuracy: 0.5400\n",
      "Epoch 84/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.7719\n",
      "Epoch 84: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4937 - accuracy: 0.7719 - val_loss: 1.0816 - val_accuracy: 0.5075\n",
      "Epoch 85/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4921 - accuracy: 0.7749\n",
      "Epoch 85: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.4936 - accuracy: 0.7731 - val_loss: 1.3538 - val_accuracy: 0.5000\n",
      "Epoch 86/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.5116 - accuracy: 0.7606\n",
      "Epoch 86: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5105 - accuracy: 0.7631 - val_loss: 1.2064 - val_accuracy: 0.5275\n",
      "Epoch 87/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5048 - accuracy: 0.7596\n",
      "Epoch 87: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.5066 - accuracy: 0.7575 - val_loss: 0.9255 - val_accuracy: 0.5150\n",
      "Epoch 88/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4880 - accuracy: 0.7774\n",
      "Epoch 88: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.4865 - accuracy: 0.7794 - val_loss: 1.0401 - val_accuracy: 0.5125\n",
      "Epoch 89/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5087 - accuracy: 0.7749\n",
      "Epoch 89: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5105 - accuracy: 0.7731 - val_loss: 1.2188 - val_accuracy: 0.5200\n",
      "Epoch 90/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.5119 - accuracy: 0.7532\n",
      "Epoch 90: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.5095 - accuracy: 0.7556 - val_loss: 1.0203 - val_accuracy: 0.5100\n",
      "Epoch 91/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4700 - accuracy: 0.7925\n",
      "Epoch 91: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4700 - accuracy: 0.7925 - val_loss: 0.9987 - val_accuracy: 0.5200\n",
      "Epoch 92/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4516 - accuracy: 0.8021\n",
      "Epoch 92: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4521 - accuracy: 0.8012 - val_loss: 1.0712 - val_accuracy: 0.5175\n",
      "Epoch 93/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4646 - accuracy: 0.7959\n",
      "Epoch 93: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 14ms/step - loss: 0.4648 - accuracy: 0.7956 - val_loss: 1.3132 - val_accuracy: 0.4925\n",
      "Epoch 94/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4672 - accuracy: 0.7946\n",
      "Epoch 94: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4686 - accuracy: 0.7931 - val_loss: 1.0258 - val_accuracy: 0.5100\n",
      "Epoch 95/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4600 - accuracy: 0.7966\n",
      "Epoch 95: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4591 - accuracy: 0.7975 - val_loss: 1.3405 - val_accuracy: 0.5175\n",
      "Epoch 96/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4749 - accuracy: 0.7945\n",
      "Epoch 96: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4757 - accuracy: 0.7919 - val_loss: 1.3503 - val_accuracy: 0.5225\n",
      "Epoch 97/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4399 - accuracy: 0.8145\n",
      "Epoch 97: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4389 - accuracy: 0.8150 - val_loss: 0.8660 - val_accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4266 - accuracy: 0.8195\n",
      "Epoch 98: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4298 - accuracy: 0.8169 - val_loss: 1.2030 - val_accuracy: 0.5200\n",
      "Epoch 99/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4401 - accuracy: 0.8178\n",
      "Epoch 99: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4415 - accuracy: 0.8194 - val_loss: 1.0770 - val_accuracy: 0.5150\n",
      "Epoch 100/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4296 - accuracy: 0.8223\n",
      "Epoch 100: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4319 - accuracy: 0.8194 - val_loss: 1.1096 - val_accuracy: 0.5300\n",
      "Epoch 101/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4133 - accuracy: 0.8256\n",
      "Epoch 101: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4133 - accuracy: 0.8256 - val_loss: 1.0885 - val_accuracy: 0.5100\n",
      "Epoch 102/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.4392 - accuracy: 0.8156\n",
      "Epoch 102: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4392 - accuracy: 0.8156 - val_loss: 0.9487 - val_accuracy: 0.5100\n",
      "Epoch 103/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.4209 - accuracy: 0.8216\n",
      "Epoch 103: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4184 - accuracy: 0.8231 - val_loss: 1.9882 - val_accuracy: 0.5250\n",
      "Epoch 104/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3933 - accuracy: 0.8411\n",
      "Epoch 104: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3950 - accuracy: 0.8388 - val_loss: 1.9448 - val_accuracy: 0.5400\n",
      "Epoch 105/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.3782 - accuracy: 0.8496\n",
      "Epoch 105: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3767 - accuracy: 0.8494 - val_loss: 2.2634 - val_accuracy: 0.5475\n",
      "Epoch 106/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3591 - accuracy: 0.8673\n",
      "Epoch 106: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3585 - accuracy: 0.8675 - val_loss: 2.1238 - val_accuracy: 0.5400\n",
      "Epoch 107/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4124 - accuracy: 0.8425\n",
      "Epoch 107: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.4163 - accuracy: 0.8394 - val_loss: 0.8694 - val_accuracy: 0.4950\n",
      "Epoch 108/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.4289 - accuracy: 0.8191\n",
      "Epoch 108: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4254 - accuracy: 0.8219 - val_loss: 1.3217 - val_accuracy: 0.5375\n",
      "Epoch 109/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3876 - accuracy: 0.8533\n",
      "Epoch 109: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3861 - accuracy: 0.8537 - val_loss: 1.3250 - val_accuracy: 0.5475\n",
      "Epoch 110/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3561 - accuracy: 0.8657\n",
      "Epoch 110: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3588 - accuracy: 0.8650 - val_loss: 2.1194 - val_accuracy: 0.5200\n",
      "Epoch 111/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3515 - accuracy: 0.8712\n",
      "Epoch 111: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3515 - accuracy: 0.8712 - val_loss: 1.2575 - val_accuracy: 0.5200\n",
      "Epoch 112/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3515 - accuracy: 0.8743\n",
      "Epoch 112: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3516 - accuracy: 0.8737 - val_loss: 1.7753 - val_accuracy: 0.5275\n",
      "Epoch 113/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3727 - accuracy: 0.8622\n",
      "Epoch 113: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3730 - accuracy: 0.8619 - val_loss: 1.2213 - val_accuracy: 0.5125\n",
      "Epoch 114/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3602 - accuracy: 0.8584\n",
      "Epoch 114: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3600 - accuracy: 0.8587 - val_loss: 1.4141 - val_accuracy: 0.5125\n",
      "Epoch 115/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.3508 - accuracy: 0.8750\n",
      "Epoch 115: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3506 - accuracy: 0.8744 - val_loss: 0.9030 - val_accuracy: 0.5025\n",
      "Epoch 116/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.3723 - accuracy: 0.8548\n",
      "Epoch 116: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3688 - accuracy: 0.8569 - val_loss: 1.2492 - val_accuracy: 0.5250\n",
      "Epoch 117/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3359 - accuracy: 0.8744\n",
      "Epoch 117: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3357 - accuracy: 0.8750 - val_loss: 1.2187 - val_accuracy: 0.5375\n",
      "Epoch 118/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.3177 - accuracy: 0.8880\n",
      "Epoch 118: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3180 - accuracy: 0.8881 - val_loss: 1.6405 - val_accuracy: 0.5300\n",
      "Epoch 119/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3216 - accuracy: 0.8888\n",
      "Epoch 119: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3216 - accuracy: 0.8888 - val_loss: 1.2351 - val_accuracy: 0.5225\n",
      "Epoch 120/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3022 - accuracy: 0.9005\n",
      "Epoch 120: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3013 - accuracy: 0.9006 - val_loss: 1.5985 - val_accuracy: 0.5025\n",
      "Epoch 121/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3564 - accuracy: 0.8619\n",
      "Epoch 121: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3564 - accuracy: 0.8619 - val_loss: 0.9196 - val_accuracy: 0.5275\n",
      "Epoch 122/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3777 - accuracy: 0.8725\n",
      "Epoch 122: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3777 - accuracy: 0.8725 - val_loss: 1.0929 - val_accuracy: 0.5050\n",
      "Epoch 123/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.3405 - accuracy: 0.8776\n",
      "Epoch 123: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3367 - accuracy: 0.8800 - val_loss: 1.0438 - val_accuracy: 0.5300\n",
      "Epoch 124/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3021 - accuracy: 0.9006\n",
      "Epoch 124: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.3021 - accuracy: 0.9006 - val_loss: 1.2148 - val_accuracy: 0.5425\n",
      "Epoch 125/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2754 - accuracy: 0.9147\n",
      "Epoch 125: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2733 - accuracy: 0.9162 - val_loss: 1.1168 - val_accuracy: 0.5450\n",
      "Epoch 126/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2717 - accuracy: 0.9133\n",
      "Epoch 126: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2709 - accuracy: 0.9137 - val_loss: 1.3106 - val_accuracy: 0.5275\n",
      "Epoch 127/200\n",
      "48/50 [===========================>..] - ETA: 0s - loss: 0.2698 - accuracy: 0.9102\n",
      "Epoch 127: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2677 - accuracy: 0.9119 - val_loss: 1.3506 - val_accuracy: 0.5125\n",
      "Epoch 128/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2566 - accuracy: 0.9269\n",
      "Epoch 128: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2564 - accuracy: 0.9256 - val_loss: 1.4904 - val_accuracy: 0.5225\n",
      "Epoch 129/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2641 - accuracy: 0.9169\n",
      "Epoch 129: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2641 - accuracy: 0.9169 - val_loss: 1.2226 - val_accuracy: 0.4975\n",
      "Epoch 130/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2596 - accuracy: 0.9196\n",
      "Epoch 130: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2569 - accuracy: 0.9212 - val_loss: 1.5191 - val_accuracy: 0.5225\n",
      "Epoch 131/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2431 - accuracy: 0.9269\n",
      "Epoch 131: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2431 - accuracy: 0.9269 - val_loss: 2.0418 - val_accuracy: 0.5000\n",
      "Epoch 132/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2389 - accuracy: 0.9309\n",
      "Epoch 132: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2420 - accuracy: 0.9287 - val_loss: 1.3169 - val_accuracy: 0.5100\n",
      "Epoch 133/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2238 - accuracy: 0.9381\n",
      "Epoch 133: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2238 - accuracy: 0.9381 - val_loss: 1.8167 - val_accuracy: 0.5100\n",
      "Epoch 134/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2261 - accuracy: 0.9349\n",
      "Epoch 134: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2289 - accuracy: 0.9331 - val_loss: 1.6164 - val_accuracy: 0.5150\n",
      "Epoch 135/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2128 - accuracy: 0.9408\n",
      "Epoch 135: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 15ms/step - loss: 0.2159 - accuracy: 0.9388 - val_loss: 1.9112 - val_accuracy: 0.5150\n",
      "Epoch 136/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.9206\n",
      "Epoch 136: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3002 - accuracy: 0.9206 - val_loss: 2.8227 - val_accuracy: 0.5125\n",
      "Epoch 137/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.4271 - accuracy: 0.8386\n",
      "Epoch 137: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.4224 - accuracy: 0.8406 - val_loss: 1.2179 - val_accuracy: 0.4975\n",
      "Epoch 138/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3308 - accuracy: 0.8852\n",
      "Epoch 138: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.3303 - accuracy: 0.8856 - val_loss: 1.1331 - val_accuracy: 0.4975\n",
      "Epoch 139/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.3250 - accuracy: 0.8782\n",
      "Epoch 139: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.3258 - accuracy: 0.8775 - val_loss: 1.5477 - val_accuracy: 0.5100\n",
      "Epoch 140/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2608 - accuracy: 0.9162\n",
      "Epoch 140: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2608 - accuracy: 0.9162 - val_loss: 1.3902 - val_accuracy: 0.5025\n",
      "Epoch 141/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2189 - accuracy: 0.9401\n",
      "Epoch 141: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 17ms/step - loss: 0.2194 - accuracy: 0.9400 - val_loss: 1.4394 - val_accuracy: 0.5075\n",
      "Epoch 142/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2103 - accuracy: 0.9431\n",
      "Epoch 142: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2103 - accuracy: 0.9431 - val_loss: 1.8832 - val_accuracy: 0.5025\n",
      "Epoch 143/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2020 - accuracy: 0.9419\n",
      "Epoch 143: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2020 - accuracy: 0.9419 - val_loss: 1.6872 - val_accuracy: 0.4925\n",
      "Epoch 144/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1974 - accuracy: 0.9494\n",
      "Epoch 144: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1974 - accuracy: 0.9494 - val_loss: 1.2024 - val_accuracy: 0.5000\n",
      "Epoch 145/200\n",
      "47/50 [===========================>..] - ETA: 0s - loss: 0.2176 - accuracy: 0.9395\n",
      "Epoch 145: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2168 - accuracy: 0.9394 - val_loss: 1.4058 - val_accuracy: 0.4950\n",
      "Epoch 146/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2060 - accuracy: 0.9439\n",
      "Epoch 146: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2057 - accuracy: 0.9438 - val_loss: 1.1984 - val_accuracy: 0.5025\n",
      "Epoch 147/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1847 - accuracy: 0.9519\n",
      "Epoch 147: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1847 - accuracy: 0.9519 - val_loss: 1.5861 - val_accuracy: 0.4925\n",
      "Epoch 148/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.1642 - accuracy: 0.9613\n",
      "Epoch 148: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.1642 - accuracy: 0.9613 - val_loss: 1.6993 - val_accuracy: 0.5100\n",
      "Epoch 149/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2226 - accuracy: 0.9444\n",
      "Epoch 149: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2226 - accuracy: 0.9444 - val_loss: 1.9716 - val_accuracy: 0.5275\n",
      "Epoch 150/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2602 - accuracy: 0.9235\n",
      "Epoch 150: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2606 - accuracy: 0.9231 - val_loss: 1.3748 - val_accuracy: 0.4975\n",
      "Epoch 151/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9281\n",
      "Epoch 151: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2384 - accuracy: 0.9281 - val_loss: 1.2167 - val_accuracy: 0.5025\n",
      "Epoch 152/200\n",
      "49/50 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9343\n",
      "Epoch 152: val_accuracy did not improve from 0.55250\n",
      "50/50 [==============================] - 1s 16ms/step - loss: 0.2309 - accuracy: 0.9344 - val_loss: 1.3329 - val_accuracy: 0.5000\n",
      "Epoch 153/200\n",
      "50/50 [==============================] - ETA: 0s - loss: 0.2329 - accuracy: 0.9319"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_540185/2190867641.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/gpu:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     H=model.fit(X_train,yTrain,\n\u001b[0m\u001b[1;32m      3\u001b[0m             \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1604\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m                         )\n\u001b[0;32m-> 1606\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1607\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1608\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1937\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1938\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1939\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1940\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1941\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1307\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1308\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 499\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    500\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    694\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 696\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    719\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 721\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3407\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3409\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3410\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3411\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device(\"/gpu:0\"):\n",
    "    H=model.fit(X_train,yTrain,\n",
    "            validation_data=(X_test, yTest),\n",
    "            epochs=200,batch_size=32,\n",
    "            callbacks=callbacks_list,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f66d007f850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 200\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "# plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_accuracy\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"test_accuracy\")\n",
    "plt.title(\"Qubit State Classification CRNN 1s (4ns SR)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
